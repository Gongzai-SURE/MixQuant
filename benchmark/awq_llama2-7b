
2025-04-13 15:22:32.187 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-13 15:22:32.188 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-13 15:22:32.679 | INFO     | __main__:<module>:119 - Start evaluating original model ...
2025-04-13 15:23:19.851 | INFO     | __main__:<module>:121 - Original model Perplexity: 7.844630241394043,using time: 47.17 seconds
2025-04-13 15:23:19.920 | INFO     | __main__:<module>:126 - Start quantization ...
2025-04-13 15:31:02.079 | INFO     | __main__:<module>:135 - Quantization time: 462.16 seconds
2025-04-13 15:31:12.705 | INFO     | __main__:<module>:141 - Start evaluating quantized model ...
2025-04-13 15:57:09.022 | INFO     | __main__:<module>:144 - AWQ quantized model Perplexity: 8.048624992370605, using time: 56.12 seconds
2025-04-13 16:04:53.680 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-13 16:04:53.681 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-13 16:04:59.069 | INFO     | __main__:<module>:142 - Start evaluating quantized model ...
2025-04-13 16:05:49.079 | INFO     | __main__:<module>:144 - AWQ quantized model Perplexity: 8.048624992370605, using time: 55.27 seconds
2025-04-20 09:31:15.048 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-20 09:31:15.049 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 8, 'version': 'GEMM'}
2025-04-20 09:31:15.785 | INFO     | __main__:<module>:119 - Start evaluating original model ...
2025-04-20 09:32:03.722 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-20 09:32:03.722 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 8, 'version': 'GEMM'}
2025-04-20 09:32:04.233 | INFO     | __main__:<module>:126 - Start quantization ...
2025-04-20 09:33:10.863 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-20 09:33:10.864 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 3, 'version': 'GEMM'}
2025-04-20 09:33:11.454 | INFO     | __main__:<module>:126 - Start quantization ...
2025-04-20 09:53:42.692 | INFO     | __main__:<module>:104 - Loading llama2-7b model ...
2025-04-20 09:53:42.693 | INFO     | __main__:<module>:108 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-20 09:53:43.283 | INFO     | __main__:<module>:126 - Start quantization ...
2025-04-26 11:29:06.197 | INFO     | __main__:<module>:110 - Loading llama2-7b model ...
2025-04-26 11:29:07.333 | INFO     | __main__:<module>:127 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-26 11:29:07.333 | INFO     | __main__:<module>:139 - Start quantization ...
2025-04-26 11:32:01.189 | INFO     | __main__:<module>:111 - Loading llama2-7b model ...
2025-04-26 11:32:02.182 | INFO     | __main__:<module>:128 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-26 11:32:02.183 | INFO     | __main__:<module>:140 - Start quantization ...
2025-04-26 11:33:05.383 | INFO     | __main__:<module>:111 - Loading llama2-7b model ...
2025-04-26 11:33:06.353 | INFO     | __main__:<module>:128 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-26 11:33:06.354 | INFO     | __main__:<module>:140 - Start quantization ...
2025-04-26 11:36:09.489 | INFO     | __main__:<module>:111 - Loading llama2-7b model ...
2025-04-26 11:36:10.714 | INFO     | __main__:<module>:128 - Quantization config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}
2025-04-26 11:36:10.714 | INFO     | __main__:<module>:140 - Start quantization ...
