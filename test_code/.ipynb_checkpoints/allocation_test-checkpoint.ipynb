{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "311e77ed-20ee-4425-9c00-9af12030c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from allocate import *\n",
    "\n",
    "fisher_info = load_json('/root/autodl-tmp/data.json')\n",
    "\n",
    "bits = [2,4,8]\n",
    "target_bits = 4.3\n",
    "layers_num = len(fisher_info)*len(fisher_info[0])\n",
    "\n",
    "allocation_strategy = Greedy_allocation_list(bits, target_bits, layers_num)\n",
    "res = get_bits_list(fisher_info,allocation_strategy)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f1cd879-57f9-45d7-a93f-7fc33d80926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_FI_layer(data):\n",
    "    from collections import defaultdict\n",
    "    values_dict = defaultdict(list)\n",
    "    for block in data:\n",
    "        for key, value in block.items():\n",
    "            values_dict[key].append(value)\n",
    "\n",
    "    rank_dict = {}\n",
    "    for key, values in values_dict.items():\n",
    "        sorted_values = sorted(values, reverse=True)\n",
    "        ranks = {v: i + 1 for i, v in enumerate(sorted_values)}\n",
    "        rank_dict[key] = [ranks[v] for v in values]\n",
    "    return rank_dict\n",
    "\n",
    "def sort_FI_in_all(data):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    all_values = []\n",
    "    for index, block in enumerate(data):\n",
    "        for key, value in block.items():\n",
    "            all_values.append((f\"{index}_{key}\", value)) \n",
    "\n",
    "    sorted_values = sorted(all_values, key=lambda x: x[1], reverse=False)\n",
    "    rank_dict = {}\n",
    "    for rank, (combined_key, _) in enumerate(sorted_values, start=1):\n",
    "        index, key = combined_key.split('_', 1)  \n",
    "        rank_dict[(int(index), key)] = rank  \n",
    "\n",
    "    result = defaultdict(dict)\n",
    "    for (index, key), rank in rank_dict.items():\n",
    "        result[index][key] = rank\n",
    "\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fc1ba10-ae3f-4d81-b473-64b11894c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'self_attn.q_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.o_proj.weight': 2, 'self_attn.v_proj.weight': 2}, {'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.o_proj.weight': 2, 'self_attn.v_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.o_proj.weight': 2, 'self_attn.q_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.o_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.o_proj.weight': 2, 'self_attn.k_proj.weight': 2, 'self_attn.q_proj.weight': 2}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.o_proj.weight': 2, 'self_attn.v_proj.weight': 2, 'self_attn.q_proj.weight': 2, 'self_attn.k_proj.weight': 4}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.q_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 2, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.q_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 4, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.q_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.q_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.q_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 2, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 8, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 2, 'mlp.up_proj.weight': 8, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 8, 'mlp.up_proj.weight': 8, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 8, 'mlp.up_proj.weight': 8, 'mlp.gate_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8}, {'mlp.down_proj.weight': 8, 'mlp.up_proj.weight': 8, 'mlp.gate_proj.weight': 8, 'self_attn.q_proj.weight': 8, 'self_attn.k_proj.weight': 8, 'self_attn.v_proj.weight': 8, 'self_attn.o_proj.weight': 8}]\n"
     ]
    }
   ],
   "source": [
    "# 判断当前排序的位宽结果\n",
    "def judge_bits(rank,allocation_strategy):\n",
    "    total_nums = 0\n",
    "    for bit,nums in allocation_strategy.items():\n",
    "        total_nums += nums\n",
    "        if rank < total_nums or rank == total_nums:\n",
    "            return bit\n",
    "\n",
    "def get_bits_list(fisher_info, strategy):\n",
    "    sorted_FI = sort_FI_in_all(fisher_info)\n",
    "    res = []\n",
    "    for index,layer_FI_rank in sorted_FI.items():\n",
    "        layers_bit = {}\n",
    "        for layer_name,FI_rank in layer_FI_rank.items():\n",
    "            bit = judge_bits(FI_rank,strategy)\n",
    "            layers_bit[layer_name] = bit\n",
    "        res.append(layers_bit)\n",
    "    return res\n",
    "\n",
    "res = get_bits_list(fisher_info, allocation_strategy)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e4829-c57b-4588-a37e-ed1bb92202bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
