{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bb901a-8ea2-4c96-84a7-365ec6dad5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 定义更复杂的 LSTM-based 策略网络\n",
    "class ComplexLSTMPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.2):\n",
    "        super(ComplexLSTMPolicyNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 双向 LSTM 层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers, \n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 全连接层，输出每个可能的位宽的概率\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 双向 LSTM 的输出维度是 hidden_dim * 2\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM 前向传播\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # 取最后一个时间步的输出，并去掉中间的维度\n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        # 应用 Dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 全连接层\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # 使用 softmax 生成概率分布\n",
    "        prob_dist = torch.softmax(out, dim=-1)\n",
    "        \n",
    "        return prob_dist, hidden\n",
    "\n",
    "# 定义强化学习环境\n",
    "class QuantizationEnv:\n",
    "    def __init__(self, layers, fisher_info, bits, p_all, R, alpha, B):\n",
    "        self.layers = layers\n",
    "        self.fisher_info = fisher_info\n",
    "        self.bits = bits\n",
    "        self.p_all = p_all\n",
    "        self.R = R\n",
    "        self.alpha = alpha\n",
    "        self.B = B\n",
    "        self.p_comp = R * p_all\n",
    "        self.current_layer = 0\n",
    "        self.assigned_bits = []\n",
    "        self.remaining_budget = self.p_comp\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_layer = 0\n",
    "        self.assigned_bits = []\n",
    "        self.remaining_budget = self.p_comp\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # 状态包括当前层的索引、已分配的位宽、剩余的参数预算\n",
    "        state = [self.current_layer, self.remaining_budget]\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 获取当前层的位宽\n",
    "        bit_width = self.bits[action]\n",
    "        \n",
    "        # 计算当前层的参数数量\n",
    "        p_i = self.layers[self.current_layer]\n",
    "        \n",
    "        # 更新剩余的参数预算\n",
    "        self.remaining_budget -= p_i * (bit_width / self.B)\n",
    "        \n",
    "        # 记录已分配的位宽\n",
    "        self.assigned_bits.append(bit_width)\n",
    "        \n",
    "        # 计算奖励（负的精度损失）\n",
    "        delta_acc = self.fisher_info[self.current_layer] * np.exp(-self.alpha * (self.B / bit_width))\n",
    "        reward = -delta_acc\n",
    "        \n",
    "        # 如果超出预算，引入惩罚\n",
    "        if self.remaining_budget < 0:\n",
    "            reward -= 10  # 惩罚项\n",
    "        \n",
    "        # 检查是否完成所有层的分配\n",
    "        done = (self.current_layer == len(self.layers) - 1)\n",
    "        \n",
    "        # 更新当前层\n",
    "        self.current_layer += 1\n",
    "        \n",
    "        # 获取下一个状态\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "# 定义强化学习类\n",
    "class RL:\n",
    "    def __init__(self, bits, F, P, N, B, R, alpha, input_dim=2, hidden_dim=256, output_dim=None, num_layers=3, lr=1e-3, gamma=0.99, batch_size=64, dropout=0.2):\n",
    "        # 初始化环境参数\n",
    "        self.bits = bits\n",
    "        self.F = F\n",
    "        self.P = P\n",
    "        self.N = N\n",
    "        self.B = B\n",
    "        self.R = R\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # 初始化强化学习超参数\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim if output_dim is not None else len(bits)\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 初始化环境\n",
    "        self.env = QuantizationEnv(self.P, self.F, self.bits, sum(self.P), self.R, self.alpha, self.B)\n",
    "        \n",
    "        # 初始化 Agent\n",
    "        self.agent = self._init_agent()\n",
    "        \n",
    "        # 学习率调度器\n",
    "        self.scheduler = ReduceLROnPlateau(self.agent.optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    def _init_agent(self):\n",
    "        return RLAgent(self.input_dim, self.hidden_dim, self.output_dim, self.num_layers, self.lr, self.gamma, self.dropout)\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        reward_history = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            hidden = None\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # 选择动作\n",
    "                action, hidden = self.agent.select_action(state, hidden)\n",
    "                \n",
    "                # 执行动作\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # 存储转移\n",
    "                self.agent.store_transition(state, action, reward, next_state, done)\n",
    "                \n",
    "                # 更新状态\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                # 训练 Agent\n",
    "                loss = self.agent.train(self.batch_size)\n",
    "                \n",
    "                # 更新学习率\n",
    "                if loss is not None:\n",
    "                    self.scheduler.step(loss)\n",
    "            \n",
    "            # 记录 Reward\n",
    "            reward_history.append(total_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Loss: {loss if loss is not None else 'N/A'}\")\n",
    "        \n",
    "        return reward_history\n",
    "    \n",
    "    def test(self):\n",
    "        state = self.env.reset()\n",
    "        hidden = None\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, hidden = self.agent.select_action(state, hidden)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            state = next_state\n",
    "            print(f\"Assigned bit width: {self.bits[action]}, Remaining budget: {self.env.remaining_budget}\")\n",
    "        \n",
    "        print(\"Final assigned bit widths:\", self.env.assigned_bits)\n",
    "\n",
    "# 定义强化学习 Agent\n",
    "class RLAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, lr=1e-3, gamma=0.99, dropout=0.2):\n",
    "        self.policy_net = ComplexLSTMPolicyNetwork(input_dim, hidden_dim, output_dim, num_layers, dropout)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # 用于存储经验回放\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "    def select_action(self, state, hidden):\n",
    "        # 将状态转换为 Tensor，并增加一个时间步维度\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)  # [1, 1, input_dim]\n",
    "        \n",
    "        # 通过策略网络生成动作概率分布\n",
    "        prob_dist, hidden = self.policy_net(state, hidden)\n",
    "        \n",
    "        # 根据概率分布采样动作\n",
    "        action = torch.multinomial(prob_dist, 1).item()\n",
    "        \n",
    "        return action, hidden\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # 从经验回放中随机采样一个 batch\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states).unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # 计算当前状态的 Q 值\n",
    "        current_q_values, _ = self.policy_net(states, None)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # 计算下一个状态的 Q 值\n",
    "        next_q_values, _ = self.policy_net(next_states, None)\n",
    "        next_q_values = next_q_values.max(1)[0].detach()\n",
    "        \n",
    "        # 计算目标 Q 值\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef154e9-238a-4c11-bee0-209c56a92474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00038: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 00110: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 00121: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 00143: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 00154: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Episode 1, Total Reward: -447.79248046875, Loss: 60.77021408081055\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 00182: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 00196: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 00207: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 00218: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 00229: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 00240: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 00251: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Episode 2, Total Reward: -177.6414337158203, Loss: 4.365023136138916\n",
      "Episode 3, Total Reward: -177.6414337158203, Loss: 5.544798374176025\n",
      "Episode 4, Total Reward: -177.6414337158203, Loss: 14.87995719909668\n",
      "Episode 5, Total Reward: -177.6414337158203, Loss: 6.929600238800049\n",
      "Episode 6, Total Reward: -177.6414337158203, Loss: 1.648818850517273\n",
      "Episode 7, Total Reward: -177.6414337158203, Loss: 0.5585541129112244\n",
      "Episode 8, Total Reward: -177.6414337158203, Loss: 71.87962341308594\n",
      "Episode 9, Total Reward: -177.6414337158203, Loss: 1.0827771425247192\n",
      "Episode 10, Total Reward: -177.6414337158203, Loss: 8.072059631347656\n",
      "Episode 11, Total Reward: -177.6414337158203, Loss: 0.5534021258354187\n",
      "Episode 12, Total Reward: -177.6414337158203, Loss: 7.033290863037109\n",
      "Episode 13, Total Reward: -177.6414337158203, Loss: 13.885327339172363\n",
      "Episode 14, Total Reward: -177.6414337158203, Loss: 0.4020290970802307\n",
      "Episode 15, Total Reward: -177.6414337158203, Loss: 0.4182106852531433\n",
      "Episode 16, Total Reward: -177.6414337158203, Loss: 2.0064690113067627\n",
      "Episode 17, Total Reward: -177.6414337158203, Loss: 7.728442192077637\n",
      "Episode 18, Total Reward: -177.6414337158203, Loss: 2.2112319469451904\n",
      "Episode 19, Total Reward: -177.6414337158203, Loss: 7.555263996124268\n",
      "Episode 20, Total Reward: -177.6414337158203, Loss: 1.9974910020828247\n",
      "Episode 21, Total Reward: -177.6414337158203, Loss: 13.018887519836426\n",
      "Episode 22, Total Reward: -177.6414337158203, Loss: 6.731760501861572\n",
      "Episode 23, Total Reward: -177.6414337158203, Loss: 7.193949222564697\n",
      "Episode 24, Total Reward: -177.6414337158203, Loss: 6.691016674041748\n",
      "Episode 25, Total Reward: -177.6414337158203, Loss: 0.42279961705207825\n",
      "Episode 26, Total Reward: -177.6414337158203, Loss: 1.2522947788238525\n",
      "Episode 27, Total Reward: -177.6414337158203, Loss: 6.836406707763672\n",
      "Episode 28, Total Reward: -177.6414337158203, Loss: 8.438610076904297\n",
      "Episode 29, Total Reward: -177.6414337158203, Loss: 8.304191589355469\n",
      "Episode 30, Total Reward: -177.6414337158203, Loss: 6.758109092712402\n",
      "Episode 31, Total Reward: -177.6414337158203, Loss: 0.4616129398345947\n",
      "Episode 32, Total Reward: -177.6414337158203, Loss: 7.002354145050049\n",
      "Episode 33, Total Reward: -177.6414337158203, Loss: 0.5004622936248779\n",
      "Episode 34, Total Reward: -177.6414337158203, Loss: 0.5311569571495056\n",
      "Episode 35, Total Reward: -177.6414337158203, Loss: 0.4280288815498352\n",
      "Episode 36, Total Reward: -177.6414337158203, Loss: 13.153085708618164\n",
      "Episode 37, Total Reward: -177.6414337158203, Loss: 4.071768283843994\n",
      "Episode 38, Total Reward: -177.6414337158203, Loss: 1.0134577751159668\n",
      "Episode 39, Total Reward: -177.6414337158203, Loss: 0.920128345489502\n",
      "Episode 40, Total Reward: -177.6414337158203, Loss: 6.719265460968018\n",
      "Episode 41, Total Reward: -177.6414337158203, Loss: 0.5475203394889832\n",
      "Episode 42, Total Reward: -177.6414337158203, Loss: 0.5204549431800842\n",
      "Episode 43, Total Reward: -177.6414337158203, Loss: 0.5155453681945801\n",
      "Episode 44, Total Reward: -177.6414337158203, Loss: 0.5363332629203796\n",
      "Episode 45, Total Reward: -177.6414337158203, Loss: 6.7253499031066895\n",
      "Episode 46, Total Reward: -177.6414337158203, Loss: 0.4746149778366089\n",
      "Episode 47, Total Reward: -177.6414337158203, Loss: 10.122295379638672\n",
      "Episode 48, Total Reward: -177.6414337158203, Loss: 12.968475341796875\n",
      "Episode 49, Total Reward: -177.6414337158203, Loss: 13.299860000610352\n",
      "Episode 50, Total Reward: -177.6414337158203, Loss: 7.015581130981445\n",
      "Episode 51, Total Reward: -177.6414337158203, Loss: 0.48211148381233215\n",
      "Episode 52, Total Reward: -177.6414337158203, Loss: 0.47920161485671997\n",
      "Episode 53, Total Reward: -177.6414337158203, Loss: 2.3485684394836426\n",
      "Episode 54, Total Reward: -177.6414337158203, Loss: 0.46979278326034546\n",
      "Episode 55, Total Reward: -177.6414337158203, Loss: 2.083172082901001\n",
      "Episode 56, Total Reward: -177.6414337158203, Loss: 0.6883782744407654\n",
      "Episode 57, Total Reward: -177.6414337158203, Loss: 2.0525803565979004\n",
      "Episode 58, Total Reward: -177.6414337158203, Loss: 5.286637306213379\n",
      "Episode 59, Total Reward: -177.6414337158203, Loss: 0.5078426599502563\n",
      "Episode 60, Total Reward: -177.6414337158203, Loss: 0.49550575017929077\n",
      "Episode 61, Total Reward: -177.6414337158203, Loss: 0.40737220644950867\n",
      "Episode 62, Total Reward: -177.6414337158203, Loss: 0.441010057926178\n",
      "Episode 63, Total Reward: -177.6414337158203, Loss: 0.8411475419998169\n",
      "Episode 64, Total Reward: -177.6414337158203, Loss: 8.31495189666748\n",
      "Episode 65, Total Reward: -177.6414337158203, Loss: 0.46961063146591187\n",
      "Episode 66, Total Reward: -177.6414337158203, Loss: 6.681594371795654\n",
      "Episode 67, Total Reward: -177.6414337158203, Loss: 2.29079008102417\n",
      "Episode 68, Total Reward: -177.6414337158203, Loss: 6.730927467346191\n",
      "Episode 69, Total Reward: -177.6414337158203, Loss: 0.46216899156570435\n",
      "Episode 70, Total Reward: -177.6414337158203, Loss: 0.5316975116729736\n",
      "Episode 71, Total Reward: -177.6414337158203, Loss: 2.0622596740722656\n",
      "Episode 72, Total Reward: -177.6414337158203, Loss: 0.41758885979652405\n",
      "Episode 73, Total Reward: -177.6414337158203, Loss: 0.8013157844543457\n",
      "Episode 74, Total Reward: -177.6414337158203, Loss: 0.6794927716255188\n",
      "Episode 75, Total Reward: -177.6414337158203, Loss: 2.322383403778076\n",
      "Episode 76, Total Reward: -177.6414337158203, Loss: 13.175593376159668\n",
      "Episode 77, Total Reward: -177.6414337158203, Loss: 0.751584529876709\n",
      "Episode 78, Total Reward: -177.6414337158203, Loss: 0.4533349275588989\n",
      "Episode 79, Total Reward: -177.6414337158203, Loss: 6.777469635009766\n",
      "Episode 80, Total Reward: -177.6414337158203, Loss: 0.7797361016273499\n",
      "Episode 81, Total Reward: -177.6414337158203, Loss: 0.6588610410690308\n",
      "Episode 82, Total Reward: -177.6414337158203, Loss: 20.88710594177246\n",
      "Episode 83, Total Reward: -177.6414337158203, Loss: 0.5266650319099426\n",
      "Episode 84, Total Reward: -177.6414337158203, Loss: 2.0625224113464355\n",
      "Episode 85, Total Reward: -177.6414337158203, Loss: 0.4890160858631134\n",
      "Episode 86, Total Reward: -177.6414337158203, Loss: 2.0235202312469482\n",
      "Episode 87, Total Reward: -177.6414337158203, Loss: 0.822723388671875\n",
      "Episode 88, Total Reward: -177.6414337158203, Loss: 1.9884809255599976\n",
      "Episode 89, Total Reward: -177.6414337158203, Loss: 2.162562608718872\n",
      "Episode 90, Total Reward: -177.6414337158203, Loss: 6.733551979064941\n",
      "Episode 91, Total Reward: -177.6414337158203, Loss: 13.104998588562012\n",
      "Episode 92, Total Reward: -177.6414337158203, Loss: 0.9658821225166321\n",
      "Episode 93, Total Reward: -177.6414337158203, Loss: 0.47200536727905273\n",
      "Episode 94, Total Reward: -177.6414337158203, Loss: 8.389888763427734\n",
      "Episode 95, Total Reward: -177.6414337158203, Loss: 0.4955815374851227\n",
      "Episode 96, Total Reward: -177.6414337158203, Loss: 0.9842609763145447\n",
      "Episode 97, Total Reward: -177.6414337158203, Loss: 0.4804934859275818\n",
      "Episode 98, Total Reward: -177.6414337158203, Loss: 6.977438449859619\n",
      "Episode 99, Total Reward: -177.6414337158203, Loss: 0.5342639684677124\n",
      "Episode 100, Total Reward: -177.6414337158203, Loss: 2.0931625366210938\n",
      "Episode 101, Total Reward: -177.6414337158203, Loss: 0.5218192934989929\n",
      "Episode 102, Total Reward: -177.6414337158203, Loss: 9.951952934265137\n",
      "Episode 103, Total Reward: -177.6414337158203, Loss: 0.4877549409866333\n",
      "Episode 104, Total Reward: -177.6414337158203, Loss: 2.14217472076416\n",
      "Episode 105, Total Reward: -177.6414337158203, Loss: 0.5211696624755859\n",
      "Episode 106, Total Reward: -177.6414337158203, Loss: 0.664350152015686\n",
      "Episode 107, Total Reward: -177.6414337158203, Loss: 6.714962005615234\n",
      "Episode 108, Total Reward: -177.6414337158203, Loss: 0.4470118284225464\n",
      "Episode 109, Total Reward: -177.6414337158203, Loss: 6.761939525604248\n",
      "Episode 110, Total Reward: -177.6414337158203, Loss: 6.836333274841309\n",
      "Episode 111, Total Reward: -177.6414337158203, Loss: 0.8424052596092224\n",
      "Episode 112, Total Reward: -177.6414337158203, Loss: 6.691508769989014\n",
      "Episode 113, Total Reward: -177.6414337158203, Loss: 0.7461105585098267\n",
      "Episode 114, Total Reward: -177.6414337158203, Loss: 2.043901205062866\n",
      "Episode 115, Total Reward: -177.6414337158203, Loss: 0.511887788772583\n",
      "Episode 116, Total Reward: -177.6414337158203, Loss: 0.7707701921463013\n",
      "Episode 117, Total Reward: -177.6414337158203, Loss: 0.6006529331207275\n",
      "Episode 118, Total Reward: -177.6414337158203, Loss: 0.677933931350708\n",
      "Episode 119, Total Reward: -177.6414337158203, Loss: 2.492466449737549\n",
      "Episode 120, Total Reward: -177.6414337158203, Loss: 2.3141536712646484\n",
      "Episode 121, Total Reward: -177.6414337158203, Loss: 0.4347884953022003\n",
      "Episode 122, Total Reward: -177.6414337158203, Loss: 1.0054508447647095\n",
      "Episode 123, Total Reward: -177.6414337158203, Loss: 0.45088672637939453\n",
      "Episode 124, Total Reward: -177.6414337158203, Loss: 0.5121452808380127\n",
      "Episode 125, Total Reward: -177.6414337158203, Loss: 2.0191173553466797\n",
      "Episode 126, Total Reward: -177.6414337158203, Loss: 6.721277713775635\n",
      "Episode 127, Total Reward: -177.6414337158203, Loss: 0.5246412754058838\n",
      "Episode 128, Total Reward: -177.6414337158203, Loss: 0.5025750994682312\n",
      "Episode 129, Total Reward: -177.6414337158203, Loss: 0.6846904754638672\n",
      "Episode 130, Total Reward: -177.6414337158203, Loss: 2.0491974353790283\n",
      "Episode 131, Total Reward: -177.6414337158203, Loss: 0.5194440484046936\n",
      "Episode 132, Total Reward: -177.6414337158203, Loss: 0.7552865147590637\n",
      "Episode 133, Total Reward: -177.6414337158203, Loss: 2.1040377616882324\n",
      "Episode 134, Total Reward: -177.6414337158203, Loss: 0.6194636225700378\n",
      "Episode 135, Total Reward: -177.6414337158203, Loss: 0.5126846432685852\n",
      "Episode 136, Total Reward: -177.6414337158203, Loss: 8.566149711608887\n",
      "Episode 137, Total Reward: -177.6414337158203, Loss: 8.376091957092285\n",
      "Episode 138, Total Reward: -177.6414337158203, Loss: 8.625570297241211\n",
      "Episode 139, Total Reward: -177.6414337158203, Loss: 0.4813208281993866\n",
      "Episode 140, Total Reward: -177.6414337158203, Loss: 0.48652929067611694\n",
      "Episode 141, Total Reward: -177.6414337158203, Loss: 14.81102180480957\n",
      "Episode 142, Total Reward: -177.6414337158203, Loss: 8.435808181762695\n",
      "Episode 143, Total Reward: -177.6414337158203, Loss: 8.361310005187988\n",
      "Episode 144, Total Reward: -177.6414337158203, Loss: 3.6994788646698\n",
      "Episode 145, Total Reward: -177.6414337158203, Loss: 0.45533835887908936\n",
      "Episode 146, Total Reward: -177.6414337158203, Loss: 2.005289316177368\n",
      "Episode 147, Total Reward: -177.6414337158203, Loss: 0.7261752486228943\n",
      "Episode 148, Total Reward: -177.6414337158203, Loss: 0.4603637456893921\n",
      "Episode 149, Total Reward: -177.6414337158203, Loss: 6.772529125213623\n",
      "Episode 150, Total Reward: -177.6414337158203, Loss: 0.44696396589279175\n",
      "Episode 151, Total Reward: -177.6414337158203, Loss: 6.830122470855713\n",
      "Episode 152, Total Reward: -177.6414337158203, Loss: 0.42505159974098206\n",
      "Episode 153, Total Reward: -177.6414337158203, Loss: 0.4757242798805237\n",
      "Episode 154, Total Reward: -177.6414337158203, Loss: 0.48960942029953003\n",
      "Episode 155, Total Reward: -177.6414337158203, Loss: 0.7659361362457275\n",
      "Episode 156, Total Reward: -177.6414337158203, Loss: 6.862736225128174\n",
      "Episode 157, Total Reward: -177.6414337158203, Loss: 0.4638882279396057\n",
      "Episode 158, Total Reward: -177.6414337158203, Loss: 0.4014580249786377\n",
      "Episode 159, Total Reward: -177.6414337158203, Loss: 0.4561954736709595\n",
      "Episode 160, Total Reward: -177.6414337158203, Loss: 2.098310947418213\n",
      "Episode 161, Total Reward: -177.6414337158203, Loss: 13.058640480041504\n",
      "Episode 162, Total Reward: -177.6414337158203, Loss: 8.436902046203613\n",
      "Episode 163, Total Reward: -177.6414337158203, Loss: 0.49385693669319153\n",
      "Episode 164, Total Reward: -177.6414337158203, Loss: 0.5104799866676331\n",
      "Episode 165, Total Reward: -177.6414337158203, Loss: 2.1180105209350586\n",
      "Episode 166, Total Reward: -177.6414337158203, Loss: 8.430463790893555\n",
      "Episode 167, Total Reward: -177.6414337158203, Loss: 0.6768638491630554\n",
      "Episode 168, Total Reward: -177.6414337158203, Loss: 0.5648112893104553\n",
      "Episode 169, Total Reward: -177.6414337158203, Loss: 0.3873610496520996\n",
      "Episode 170, Total Reward: -177.6414337158203, Loss: 6.713235378265381\n",
      "Episode 171, Total Reward: -177.6414337158203, Loss: 0.5471338033676147\n",
      "Episode 172, Total Reward: -177.6414337158203, Loss: 0.5335066318511963\n",
      "Episode 173, Total Reward: -177.6414337158203, Loss: 2.06154465675354\n",
      "Episode 174, Total Reward: -177.6414337158203, Loss: 6.787154674530029\n",
      "Episode 175, Total Reward: -177.6414337158203, Loss: 0.495347797870636\n",
      "Episode 176, Total Reward: -177.6414337158203, Loss: 0.9303482174873352\n",
      "Episode 177, Total Reward: -177.6414337158203, Loss: 2.30952787399292\n",
      "Episode 178, Total Reward: -177.6414337158203, Loss: 6.713030815124512\n",
      "Episode 179, Total Reward: -177.6414337158203, Loss: 0.6978263258934021\n",
      "Episode 180, Total Reward: -177.6414337158203, Loss: 2.0296597480773926\n",
      "Episode 181, Total Reward: -177.6414337158203, Loss: 0.41365569829940796\n",
      "Episode 182, Total Reward: -177.6414337158203, Loss: 2.048753261566162\n",
      "Episode 183, Total Reward: -177.6414337158203, Loss: 6.826592445373535\n",
      "Episode 184, Total Reward: -177.6414337158203, Loss: 2.2577779293060303\n",
      "Episode 185, Total Reward: -177.6414337158203, Loss: 6.806347846984863\n",
      "Episode 186, Total Reward: -177.6414337158203, Loss: 0.5433895587921143\n",
      "Episode 187, Total Reward: -177.6414337158203, Loss: 0.5878028869628906\n",
      "Episode 188, Total Reward: -177.6414337158203, Loss: 8.612531661987305\n",
      "Episode 189, Total Reward: -177.6414337158203, Loss: 0.950955867767334\n",
      "Episode 190, Total Reward: -177.6414337158203, Loss: 6.742896556854248\n",
      "Episode 191, Total Reward: -177.6414337158203, Loss: 6.827855587005615\n",
      "Episode 192, Total Reward: -177.6414337158203, Loss: 0.7144240140914917\n",
      "Episode 193, Total Reward: -177.6414337158203, Loss: 0.5590615272521973\n",
      "Episode 194, Total Reward: -177.6414337158203, Loss: 12.997840881347656\n",
      "Episode 195, Total Reward: -177.6414337158203, Loss: 0.40596097707748413\n",
      "Episode 196, Total Reward: -177.6414337158203, Loss: 0.4337158799171448\n",
      "Episode 197, Total Reward: -177.6414337158203, Loss: 0.4363895356655121\n",
      "Episode 198, Total Reward: -177.6414337158203, Loss: 0.4469638764858246\n",
      "Episode 199, Total Reward: -177.6414337158203, Loss: 2.1125316619873047\n",
      "Episode 200, Total Reward: -177.6414337158203, Loss: 1.9929840564727783\n",
      "Episode 201, Total Reward: -177.6414337158203, Loss: 1.0586687326431274\n",
      "Episode 202, Total Reward: -177.6414337158203, Loss: 2.1419925689697266\n",
      "Episode 203, Total Reward: -177.6414337158203, Loss: 0.7593669891357422\n",
      "Episode 204, Total Reward: -177.6414337158203, Loss: 2.0734522342681885\n",
      "Episode 205, Total Reward: -177.6414337158203, Loss: 6.766315460205078\n",
      "Episode 206, Total Reward: -177.6414337158203, Loss: 0.5893987417221069\n",
      "Episode 207, Total Reward: -177.6414337158203, Loss: 0.5548285245895386\n",
      "Episode 208, Total Reward: -177.6414337158203, Loss: 0.5254259705543518\n",
      "Episode 209, Total Reward: -177.6414337158203, Loss: 0.466695100069046\n",
      "Episode 210, Total Reward: -177.6414337158203, Loss: 2.33894681930542\n",
      "Episode 211, Total Reward: -177.6414337158203, Loss: 0.45247283577919006\n",
      "Episode 212, Total Reward: -177.6414337158203, Loss: 0.46800148487091064\n",
      "Episode 213, Total Reward: -177.6414337158203, Loss: 0.5039904713630676\n",
      "Episode 214, Total Reward: -177.6414337158203, Loss: 0.43441611528396606\n",
      "Episode 215, Total Reward: -177.6414337158203, Loss: 0.5564521551132202\n",
      "Episode 216, Total Reward: -177.6414337158203, Loss: 2.32979154586792\n",
      "Episode 217, Total Reward: -177.6414337158203, Loss: 0.7572936415672302\n",
      "Episode 218, Total Reward: -177.6414337158203, Loss: 0.5615714192390442\n",
      "Episode 219, Total Reward: -177.6414337158203, Loss: 2.01172137260437\n",
      "Episode 220, Total Reward: -177.6414337158203, Loss: 0.5010604858398438\n",
      "Episode 221, Total Reward: -177.6414337158203, Loss: 2.1890993118286133\n",
      "Episode 222, Total Reward: -177.6414337158203, Loss: 6.803869247436523\n",
      "Episode 223, Total Reward: -177.6414337158203, Loss: 0.9968541264533997\n",
      "Episode 224, Total Reward: -177.6414337158203, Loss: 0.48021745681762695\n",
      "Episode 225, Total Reward: -177.6414337158203, Loss: 0.4610503911972046\n",
      "Episode 226, Total Reward: -177.6414337158203, Loss: 2.105567693710327\n",
      "Episode 227, Total Reward: -177.6414337158203, Loss: 8.282815933227539\n",
      "Episode 228, Total Reward: -177.6414337158203, Loss: 6.942677021026611\n",
      "Episode 229, Total Reward: -177.6414337158203, Loss: 2.0926272869110107\n",
      "Episode 230, Total Reward: -177.6414337158203, Loss: 0.5952228307723999\n",
      "Episode 231, Total Reward: -177.6414337158203, Loss: 0.5302184224128723\n",
      "Episode 232, Total Reward: -177.6414337158203, Loss: 0.5329066514968872\n",
      "Episode 233, Total Reward: -177.6414337158203, Loss: 0.5378962755203247\n",
      "Episode 234, Total Reward: -177.6414337158203, Loss: 3.926300048828125\n",
      "Episode 235, Total Reward: -177.6414337158203, Loss: 0.5173482298851013\n",
      "Episode 236, Total Reward: -177.6414337158203, Loss: 8.443708419799805\n",
      "Episode 237, Total Reward: -177.6414337158203, Loss: 0.43549278378486633\n",
      "Episode 238, Total Reward: -177.6414337158203, Loss: 2.282099723815918\n",
      "Episode 239, Total Reward: -177.6414337158203, Loss: 6.7403244972229\n",
      "Episode 240, Total Reward: -177.6414337158203, Loss: 0.4359474778175354\n",
      "Episode 241, Total Reward: -177.6414337158203, Loss: 6.80269718170166\n",
      "Episode 242, Total Reward: -177.6414337158203, Loss: 0.3970758020877838\n",
      "Episode 243, Total Reward: -177.6414337158203, Loss: 1.199031949043274\n",
      "Episode 244, Total Reward: -177.6414337158203, Loss: 0.5031348466873169\n",
      "Episode 245, Total Reward: -177.6414337158203, Loss: 6.798654556274414\n",
      "Episode 246, Total Reward: -177.6414337158203, Loss: 0.4598509669303894\n",
      "Episode 247, Total Reward: -177.6414337158203, Loss: 0.4796721935272217\n",
      "Episode 248, Total Reward: -177.6414337158203, Loss: 0.8373664617538452\n",
      "Episode 249, Total Reward: -177.6414337158203, Loss: 0.44682976603507996\n",
      "Episode 250, Total Reward: -177.6414337158203, Loss: 2.057504653930664\n",
      "Episode 251, Total Reward: -177.6414337158203, Loss: 0.4626982510089874\n",
      "Episode 252, Total Reward: -177.6414337158203, Loss: 0.4122854471206665\n",
      "Episode 253, Total Reward: -177.6414337158203, Loss: 0.7902735471725464\n",
      "Episode 254, Total Reward: -177.6414337158203, Loss: 0.7371119856834412\n",
      "Episode 255, Total Reward: -177.6414337158203, Loss: 8.420615196228027\n",
      "Episode 256, Total Reward: -177.6414337158203, Loss: 0.42679521441459656\n",
      "Episode 257, Total Reward: -177.6414337158203, Loss: 13.03154182434082\n",
      "Episode 258, Total Reward: -177.6414337158203, Loss: 0.5285817980766296\n",
      "Episode 259, Total Reward: -177.6414337158203, Loss: 0.7227098345756531\n",
      "Episode 260, Total Reward: -177.6414337158203, Loss: 0.8879162073135376\n",
      "Episode 261, Total Reward: -177.6414337158203, Loss: 13.077173233032227\n",
      "Episode 262, Total Reward: -177.6414337158203, Loss: 0.45563575625419617\n",
      "Episode 263, Total Reward: -177.6414337158203, Loss: 6.777447700500488\n",
      "Episode 264, Total Reward: -177.6414337158203, Loss: 3.9443018436431885\n",
      "Episode 265, Total Reward: -177.6414337158203, Loss: 0.7690789699554443\n",
      "Episode 266, Total Reward: -177.6414337158203, Loss: 6.950709819793701\n",
      "Episode 267, Total Reward: -177.6414337158203, Loss: 6.721248626708984\n",
      "Episode 268, Total Reward: -177.6414337158203, Loss: 2.0442967414855957\n",
      "Episode 269, Total Reward: -177.6414337158203, Loss: 0.740894079208374\n",
      "Episode 270, Total Reward: -177.6414337158203, Loss: 12.96422004699707\n",
      "Episode 271, Total Reward: -177.6414337158203, Loss: 6.705777168273926\n",
      "Episode 272, Total Reward: -177.6414337158203, Loss: 3.7826242446899414\n",
      "Episode 273, Total Reward: -177.6414337158203, Loss: 2.1234872341156006\n",
      "Episode 274, Total Reward: -177.6414337158203, Loss: 0.7251415848731995\n",
      "Episode 275, Total Reward: -177.6414337158203, Loss: 0.455629825592041\n",
      "Episode 276, Total Reward: -177.6414337158203, Loss: 0.7156020998954773\n",
      "Episode 277, Total Reward: -177.6414337158203, Loss: 0.7277405261993408\n",
      "Episode 278, Total Reward: -177.6414337158203, Loss: 0.43053480982780457\n",
      "Episode 279, Total Reward: -177.6414337158203, Loss: 6.6750078201293945\n",
      "Episode 280, Total Reward: -177.6414337158203, Loss: 2.325915813446045\n",
      "Episode 281, Total Reward: -177.6414337158203, Loss: 2.0392520427703857\n",
      "Episode 282, Total Reward: -177.6414337158203, Loss: 6.717557907104492\n",
      "Episode 283, Total Reward: -177.6414337158203, Loss: 8.354480743408203\n",
      "Episode 284, Total Reward: -177.6414337158203, Loss: 0.4391217827796936\n",
      "Episode 285, Total Reward: -177.6414337158203, Loss: 7.024486064910889\n",
      "Episode 286, Total Reward: -177.6414337158203, Loss: 0.489571213722229\n",
      "Episode 287, Total Reward: -177.6414337158203, Loss: 7.015050411224365\n",
      "Episode 288, Total Reward: -177.6414337158203, Loss: 0.4643056094646454\n",
      "Episode 289, Total Reward: -177.6414337158203, Loss: 0.4886590838432312\n",
      "Episode 290, Total Reward: -177.6414337158203, Loss: 0.48745018243789673\n",
      "Episode 291, Total Reward: -177.6414337158203, Loss: 6.972535610198975\n",
      "Episode 292, Total Reward: -177.6414337158203, Loss: 0.4301329255104065\n",
      "Episode 293, Total Reward: -177.6414337158203, Loss: 8.301335334777832\n",
      "Episode 294, Total Reward: -177.6414337158203, Loss: 2.0957446098327637\n",
      "Episode 295, Total Reward: -177.6414337158203, Loss: 6.853675842285156\n",
      "Episode 296, Total Reward: -177.6414337158203, Loss: 8.338509559631348\n",
      "Episode 297, Total Reward: -177.6414337158203, Loss: 0.4678889513015747\n",
      "Episode 298, Total Reward: -177.6414337158203, Loss: 0.44802629947662354\n",
      "Episode 299, Total Reward: -177.6414337158203, Loss: 0.5118449330329895\n",
      "Episode 300, Total Reward: -177.6414337158203, Loss: 0.5083339214324951\n",
      "Episode 301, Total Reward: -177.6414337158203, Loss: 8.45313835144043\n",
      "Episode 302, Total Reward: -177.6414337158203, Loss: 0.49981701374053955\n",
      "Episode 303, Total Reward: -177.6414337158203, Loss: 0.4597344696521759\n",
      "Episode 304, Total Reward: -177.6414337158203, Loss: 0.4231162369251251\n",
      "Episode 305, Total Reward: -177.6414337158203, Loss: 6.850481986999512\n",
      "Episode 306, Total Reward: -177.6414337158203, Loss: 0.4925832748413086\n",
      "Episode 307, Total Reward: -177.6414337158203, Loss: 9.900951385498047\n",
      "Episode 308, Total Reward: -177.6414337158203, Loss: 0.5005762577056885\n",
      "Episode 309, Total Reward: -177.6414337158203, Loss: 6.685511112213135\n",
      "Episode 310, Total Reward: -177.6414337158203, Loss: 2.580047607421875\n",
      "Episode 311, Total Reward: -177.6414337158203, Loss: 0.49737808108329773\n",
      "Episode 312, Total Reward: -177.6414337158203, Loss: 0.6818418502807617\n",
      "Episode 313, Total Reward: -177.6414337158203, Loss: 2.0627944469451904\n",
      "Episode 314, Total Reward: -177.6414337158203, Loss: 2.1217527389526367\n",
      "Episode 315, Total Reward: -177.6414337158203, Loss: 0.5337274074554443\n",
      "Episode 316, Total Reward: -177.6414337158203, Loss: 12.907915115356445\n",
      "Episode 317, Total Reward: -177.6414337158203, Loss: 0.5790238380432129\n",
      "Episode 318, Total Reward: -177.6414337158203, Loss: 0.45630621910095215\n",
      "Episode 319, Total Reward: -177.6414337158203, Loss: 0.7682023048400879\n",
      "Episode 320, Total Reward: -177.6414337158203, Loss: 0.5977976322174072\n",
      "Episode 321, Total Reward: -177.6414337158203, Loss: 14.549873352050781\n",
      "Episode 322, Total Reward: -177.6414337158203, Loss: 7.201961517333984\n",
      "Episode 323, Total Reward: -177.6414337158203, Loss: 6.955101490020752\n",
      "Episode 324, Total Reward: -177.6414337158203, Loss: 0.517273485660553\n",
      "Episode 325, Total Reward: -177.6414337158203, Loss: 19.287965774536133\n",
      "Episode 326, Total Reward: -177.6414337158203, Loss: 0.49398183822631836\n",
      "Episode 327, Total Reward: -177.6414337158203, Loss: 0.5536646842956543\n",
      "Episode 328, Total Reward: -177.6414337158203, Loss: 0.4853818714618683\n",
      "Episode 329, Total Reward: -177.6414337158203, Loss: 12.98475456237793\n",
      "Episode 330, Total Reward: -177.6414337158203, Loss: 2.122241258621216\n",
      "Episode 331, Total Reward: -177.6414337158203, Loss: 2.040536403656006\n",
      "Episode 332, Total Reward: -177.6414337158203, Loss: 2.0915443897247314\n",
      "Episode 333, Total Reward: -177.6414337158203, Loss: 6.7406325340271\n",
      "Episode 334, Total Reward: -177.6414337158203, Loss: 0.5035538673400879\n",
      "Episode 335, Total Reward: -177.6414337158203, Loss: 2.074460744857788\n",
      "Episode 336, Total Reward: -177.6414337158203, Loss: 9.975471496582031\n",
      "Episode 337, Total Reward: -177.6414337158203, Loss: 6.711822032928467\n",
      "Episode 338, Total Reward: -177.6414337158203, Loss: 0.7033994793891907\n",
      "Episode 339, Total Reward: -177.6414337158203, Loss: 8.339911460876465\n",
      "Episode 340, Total Reward: -177.6414337158203, Loss: 2.0194501876831055\n",
      "Episode 341, Total Reward: -177.6414337158203, Loss: 0.49627485871315\n",
      "Episode 342, Total Reward: -177.6414337158203, Loss: 2.1429929733276367\n",
      "Episode 343, Total Reward: -177.6414337158203, Loss: 7.297967910766602\n",
      "Episode 344, Total Reward: -177.6414337158203, Loss: 0.4832211434841156\n",
      "Episode 345, Total Reward: -177.6414337158203, Loss: 0.5570656657218933\n",
      "Episode 346, Total Reward: -177.6414337158203, Loss: 2.0950183868408203\n",
      "Episode 347, Total Reward: -177.6414337158203, Loss: 2.173351287841797\n",
      "Episode 348, Total Reward: -177.6414337158203, Loss: 2.056236743927002\n",
      "Episode 349, Total Reward: -177.6414337158203, Loss: 0.5141511559486389\n",
      "Episode 350, Total Reward: -177.6414337158203, Loss: 6.819254398345947\n",
      "Episode 351, Total Reward: -177.6414337158203, Loss: 2.0764150619506836\n",
      "Episode 352, Total Reward: -177.6414337158203, Loss: 2.1136913299560547\n",
      "Episode 353, Total Reward: -177.6414337158203, Loss: 2.0643529891967773\n",
      "Episode 354, Total Reward: -177.6414337158203, Loss: 0.7542694211006165\n",
      "Episode 355, Total Reward: -177.6414337158203, Loss: 2.0438108444213867\n",
      "Episode 356, Total Reward: -177.6414337158203, Loss: 0.7912534475326538\n",
      "Episode 357, Total Reward: -177.6414337158203, Loss: 0.46351462602615356\n",
      "Episode 358, Total Reward: -177.6414337158203, Loss: 6.655269145965576\n",
      "Episode 359, Total Reward: -177.6414337158203, Loss: 2.137613296508789\n",
      "Episode 360, Total Reward: -177.6414337158203, Loss: 2.05495023727417\n",
      "Episode 361, Total Reward: -177.6414337158203, Loss: 0.470611572265625\n",
      "Episode 362, Total Reward: -177.6414337158203, Loss: 0.436321884393692\n",
      "Episode 363, Total Reward: -177.6414337158203, Loss: 0.7280709147453308\n",
      "Episode 364, Total Reward: -177.6414337158203, Loss: 0.5282594561576843\n",
      "Episode 365, Total Reward: -177.6414337158203, Loss: 0.4390801787376404\n",
      "Episode 366, Total Reward: -177.6414337158203, Loss: 0.48680001497268677\n",
      "Episode 367, Total Reward: -177.6414337158203, Loss: 2.1742777824401855\n",
      "Episode 368, Total Reward: -177.6414337158203, Loss: 0.9498544931411743\n",
      "Episode 369, Total Reward: -177.6414337158203, Loss: 8.833415985107422\n",
      "Episode 370, Total Reward: -177.6414337158203, Loss: 2.317593574523926\n",
      "Episode 371, Total Reward: -177.6414337158203, Loss: 2.0252959728240967\n",
      "Episode 372, Total Reward: -177.6414337158203, Loss: 0.5488876700401306\n",
      "Episode 373, Total Reward: -177.6414337158203, Loss: 14.890892028808594\n",
      "Episode 374, Total Reward: -177.6414337158203, Loss: 6.876878261566162\n",
      "Episode 375, Total Reward: -177.6414337158203, Loss: 2.1413817405700684\n",
      "Episode 376, Total Reward: -177.6414337158203, Loss: 2.30051851272583\n",
      "Episode 377, Total Reward: -177.6414337158203, Loss: 2.0109362602233887\n",
      "Episode 378, Total Reward: -177.6414337158203, Loss: 0.6866326928138733\n",
      "Episode 379, Total Reward: -177.6414337158203, Loss: 2.2225451469421387\n",
      "Episode 380, Total Reward: -177.6414337158203, Loss: 2.084599494934082\n",
      "Episode 381, Total Reward: -177.6414337158203, Loss: 6.735611915588379\n",
      "Episode 382, Total Reward: -177.6414337158203, Loss: 0.4917445182800293\n",
      "Episode 383, Total Reward: -177.6414337158203, Loss: 0.49973171949386597\n",
      "Episode 384, Total Reward: -177.6414337158203, Loss: 2.2872507572174072\n",
      "Episode 385, Total Reward: -177.6414337158203, Loss: 2.04248046875\n",
      "Episode 386, Total Reward: -177.6414337158203, Loss: 2.1153972148895264\n",
      "Episode 387, Total Reward: -177.6414337158203, Loss: 0.4254453778266907\n",
      "Episode 388, Total Reward: -177.6414337158203, Loss: 0.42387816309928894\n",
      "Episode 389, Total Reward: -177.6414337158203, Loss: 0.7230074405670166\n",
      "Episode 390, Total Reward: -177.6414337158203, Loss: 0.7015151381492615\n",
      "Episode 391, Total Reward: -177.6414337158203, Loss: 0.7826595306396484\n",
      "Episode 392, Total Reward: -177.6414337158203, Loss: 0.5372835397720337\n",
      "Episode 393, Total Reward: -177.6414337158203, Loss: 6.748085021972656\n",
      "Episode 394, Total Reward: -177.6414337158203, Loss: 0.9962196350097656\n",
      "Episode 395, Total Reward: -177.6414337158203, Loss: 0.4706525504589081\n",
      "Episode 396, Total Reward: -177.6414337158203, Loss: 2.0677595138549805\n",
      "Episode 397, Total Reward: -177.6414337158203, Loss: 0.4976566433906555\n",
      "Episode 398, Total Reward: -177.6414337158203, Loss: 2.310387372970581\n",
      "Episode 399, Total Reward: -177.6414337158203, Loss: 0.4647115468978882\n",
      "Episode 400, Total Reward: -177.6414337158203, Loss: 1.9963445663452148\n",
      "Episode 401, Total Reward: -177.6414337158203, Loss: 0.40852123498916626\n",
      "Episode 402, Total Reward: -177.6414337158203, Loss: 1.3077821731567383\n",
      "Episode 403, Total Reward: -177.6414337158203, Loss: 0.790273904800415\n",
      "Episode 404, Total Reward: -177.6414337158203, Loss: 14.581303596496582\n",
      "Episode 405, Total Reward: -177.6414337158203, Loss: 0.47809821367263794\n",
      "Episode 406, Total Reward: -177.6414337158203, Loss: 0.6848707795143127\n",
      "Episode 407, Total Reward: -177.6414337158203, Loss: 0.5447160601615906\n",
      "Episode 408, Total Reward: -177.6414337158203, Loss: 0.96821129322052\n",
      "Episode 409, Total Reward: -177.6414337158203, Loss: 0.5341078639030457\n",
      "Episode 410, Total Reward: -177.6414337158203, Loss: 0.5166659951210022\n",
      "Episode 411, Total Reward: -177.6414337158203, Loss: 0.46176227927207947\n",
      "Episode 412, Total Reward: -177.6414337158203, Loss: 3.6496946811676025\n",
      "Episode 413, Total Reward: -177.6414337158203, Loss: 2.1252918243408203\n",
      "Episode 414, Total Reward: -177.6414337158203, Loss: 0.5418312549591064\n",
      "Episode 415, Total Reward: -177.6414337158203, Loss: 0.4675518572330475\n",
      "Episode 416, Total Reward: -177.6414337158203, Loss: 3.6592888832092285\n",
      "Episode 417, Total Reward: -177.6414337158203, Loss: 6.933996200561523\n",
      "Episode 418, Total Reward: -177.6414337158203, Loss: 0.5301541090011597\n",
      "Episode 419, Total Reward: -177.6414337158203, Loss: 8.635509490966797\n",
      "Episode 420, Total Reward: -177.6414337158203, Loss: 0.4757576584815979\n",
      "Episode 421, Total Reward: -177.6414337158203, Loss: 0.5688623785972595\n",
      "Episode 422, Total Reward: -177.6414337158203, Loss: 6.724181652069092\n",
      "Episode 423, Total Reward: -177.6414337158203, Loss: 2.016742467880249\n",
      "Episode 424, Total Reward: -177.6414337158203, Loss: 0.5211591720581055\n",
      "Episode 425, Total Reward: -177.6414337158203, Loss: 6.921092987060547\n",
      "Episode 426, Total Reward: -177.6414337158203, Loss: 2.0030882358551025\n",
      "Episode 427, Total Reward: -177.6414337158203, Loss: 0.4909817576408386\n",
      "Episode 428, Total Reward: -177.6414337158203, Loss: 0.8365740180015564\n",
      "Episode 429, Total Reward: -177.6414337158203, Loss: 0.5659014582633972\n",
      "Episode 430, Total Reward: -177.6414337158203, Loss: 7.0426225662231445\n",
      "Episode 431, Total Reward: -177.6414337158203, Loss: 0.4954448938369751\n",
      "Episode 432, Total Reward: -177.6414337158203, Loss: 0.4971662163734436\n",
      "Episode 433, Total Reward: -177.6414337158203, Loss: 0.8031622171401978\n",
      "Episode 434, Total Reward: -177.6414337158203, Loss: 6.774143218994141\n",
      "Episode 435, Total Reward: -177.6414337158203, Loss: 0.430885910987854\n",
      "Episode 436, Total Reward: -177.6414337158203, Loss: 6.736595153808594\n",
      "Episode 437, Total Reward: -177.6414337158203, Loss: 0.7086004614830017\n",
      "Episode 438, Total Reward: -177.6414337158203, Loss: 0.5198063850402832\n",
      "Episode 439, Total Reward: -177.6414337158203, Loss: 2.0876972675323486\n",
      "Episode 440, Total Reward: -177.6414337158203, Loss: 6.674656867980957\n",
      "Episode 441, Total Reward: -177.6414337158203, Loss: 10.151134490966797\n",
      "Episode 442, Total Reward: -177.6414337158203, Loss: 1.9828685522079468\n",
      "Episode 443, Total Reward: -177.6414337158203, Loss: 2.047471284866333\n",
      "Episode 444, Total Reward: -177.6414337158203, Loss: 0.4666082262992859\n",
      "Episode 445, Total Reward: -177.6414337158203, Loss: 0.548223614692688\n",
      "Episode 446, Total Reward: -177.6414337158203, Loss: 10.26816177368164\n",
      "Episode 447, Total Reward: -177.6414337158203, Loss: 0.7568371295928955\n",
      "Episode 448, Total Reward: -177.6414337158203, Loss: 6.9695048332214355\n",
      "Episode 449, Total Reward: -177.6414337158203, Loss: 6.73153829574585\n",
      "Episode 450, Total Reward: -177.6414337158203, Loss: 0.5382717847824097\n",
      "Episode 451, Total Reward: -177.6414337158203, Loss: 0.5287626385688782\n",
      "Episode 452, Total Reward: -177.6414337158203, Loss: 0.7104678153991699\n",
      "Episode 453, Total Reward: -177.6414337158203, Loss: 2.1645736694335938\n",
      "Episode 454, Total Reward: -177.6414337158203, Loss: 0.6925784349441528\n",
      "Episode 455, Total Reward: -177.6414337158203, Loss: 0.44665390253067017\n",
      "Episode 456, Total Reward: -177.6414337158203, Loss: 0.44785046577453613\n",
      "Episode 457, Total Reward: -177.6414337158203, Loss: 14.549989700317383\n",
      "Episode 458, Total Reward: -177.6414337158203, Loss: 8.311824798583984\n",
      "Episode 459, Total Reward: -177.6414337158203, Loss: 0.4253343343734741\n",
      "Episode 460, Total Reward: -177.6414337158203, Loss: 0.46267932653427124\n",
      "Episode 461, Total Reward: -177.6414337158203, Loss: 0.46589064598083496\n",
      "Episode 462, Total Reward: -177.6414337158203, Loss: 0.4339824318885803\n",
      "Episode 463, Total Reward: -177.6414337158203, Loss: 0.9654773473739624\n",
      "Episode 464, Total Reward: -177.6414337158203, Loss: 2.0981783866882324\n",
      "Episode 465, Total Reward: -177.6414337158203, Loss: 2.0319716930389404\n",
      "Episode 466, Total Reward: -177.6414337158203, Loss: 0.7945723533630371\n",
      "Episode 467, Total Reward: -177.6414337158203, Loss: 8.352391242980957\n",
      "Episode 468, Total Reward: -177.6414337158203, Loss: 0.8606671094894409\n",
      "Episode 469, Total Reward: -177.6414337158203, Loss: 0.44271382689476013\n",
      "Episode 470, Total Reward: -177.6414337158203, Loss: 2.027798652648926\n",
      "Episode 471, Total Reward: -177.6414337158203, Loss: 6.9279351234436035\n",
      "Episode 472, Total Reward: -177.6414337158203, Loss: 8.36186695098877\n",
      "Episode 473, Total Reward: -177.6414337158203, Loss: 6.7664713859558105\n",
      "Episode 474, Total Reward: -177.6414337158203, Loss: 0.4383031129837036\n",
      "Episode 475, Total Reward: -177.6414337158203, Loss: 2.0140509605407715\n",
      "Episode 476, Total Reward: -177.6414337158203, Loss: 0.4642102122306824\n",
      "Episode 477, Total Reward: -177.6414337158203, Loss: 0.5414711833000183\n",
      "Episode 478, Total Reward: -177.6414337158203, Loss: 0.5596771836280823\n",
      "Episode 479, Total Reward: -177.6414337158203, Loss: 6.805820465087891\n",
      "Episode 480, Total Reward: -177.6414337158203, Loss: 0.4733579158782959\n",
      "Episode 481, Total Reward: -177.6414337158203, Loss: 0.6459534764289856\n",
      "Episode 482, Total Reward: -177.6414337158203, Loss: 8.37232780456543\n",
      "Episode 483, Total Reward: -177.6414337158203, Loss: 0.861133337020874\n",
      "Episode 484, Total Reward: -177.6414337158203, Loss: 0.5712937116622925\n",
      "Episode 485, Total Reward: -177.6414337158203, Loss: 6.673681735992432\n",
      "Episode 486, Total Reward: -177.6414337158203, Loss: 0.7243125438690186\n",
      "Episode 487, Total Reward: -177.6414337158203, Loss: 0.413989394903183\n",
      "Episode 488, Total Reward: -177.6414337158203, Loss: 6.774279594421387\n",
      "Episode 489, Total Reward: -177.6414337158203, Loss: 0.5192352533340454\n",
      "Episode 490, Total Reward: -177.6414337158203, Loss: 0.47985008358955383\n",
      "Episode 491, Total Reward: -177.6414337158203, Loss: 0.4919876158237457\n",
      "Episode 492, Total Reward: -177.6414337158203, Loss: 8.540786743164062\n",
      "Episode 493, Total Reward: -177.6414337158203, Loss: 0.48289889097213745\n",
      "Episode 494, Total Reward: -177.6414337158203, Loss: 0.5323115587234497\n",
      "Episode 495, Total Reward: -177.6414337158203, Loss: 2.0229766368865967\n",
      "Episode 496, Total Reward: -177.6414337158203, Loss: 0.5271795392036438\n",
      "Episode 497, Total Reward: -177.6414337158203, Loss: 19.313758850097656\n",
      "Episode 498, Total Reward: -177.6414337158203, Loss: 0.4764871299266815\n",
      "Episode 499, Total Reward: -177.6414337158203, Loss: 6.6587324142456055\n",
      "Episode 500, Total Reward: -177.6414337158203, Loss: 2.3062567710876465\n",
      "Assigned bit width: 2, Remaining budget: 1616904192.0\n",
      "Assigned bit width: 2, Remaining budget: 1614807040.0\n",
      "Assigned bit width: 2, Remaining budget: 1612709888.0\n",
      "Assigned bit width: 2, Remaining budget: 1610612736.0\n",
      "Assigned bit width: 2, Remaining budget: 1604976640.0\n",
      "Assigned bit width: 2, Remaining budget: 1599340544.0\n",
      "Assigned bit width: 2, Remaining budget: 1593704448.0\n",
      "Assigned bit width: 2, Remaining budget: 1591607296.0\n",
      "Assigned bit width: 2, Remaining budget: 1589510144.0\n",
      "Assigned bit width: 2, Remaining budget: 1587412992.0\n",
      "Assigned bit width: 2, Remaining budget: 1585315840.0\n",
      "Assigned bit width: 2, Remaining budget: 1579679744.0\n",
      "Assigned bit width: 2, Remaining budget: 1574043648.0\n",
      "Assigned bit width: 2, Remaining budget: 1568407552.0\n",
      "Assigned bit width: 2, Remaining budget: 1566310400.0\n",
      "Assigned bit width: 2, Remaining budget: 1564213248.0\n",
      "Assigned bit width: 2, Remaining budget: 1562116096.0\n",
      "Assigned bit width: 2, Remaining budget: 1560018944.0\n",
      "Assigned bit width: 2, Remaining budget: 1554382848.0\n",
      "Assigned bit width: 2, Remaining budget: 1548746752.0\n",
      "Assigned bit width: 2, Remaining budget: 1543110656.0\n",
      "Assigned bit width: 2, Remaining budget: 1541013504.0\n",
      "Assigned bit width: 2, Remaining budget: 1538916352.0\n",
      "Assigned bit width: 2, Remaining budget: 1536819200.0\n",
      "Assigned bit width: 2, Remaining budget: 1534722048.0\n",
      "Assigned bit width: 2, Remaining budget: 1529085952.0\n",
      "Assigned bit width: 2, Remaining budget: 1523449856.0\n",
      "Assigned bit width: 2, Remaining budget: 1517813760.0\n",
      "Assigned bit width: 2, Remaining budget: 1515716608.0\n",
      "Assigned bit width: 2, Remaining budget: 1513619456.0\n",
      "Assigned bit width: 2, Remaining budget: 1511522304.0\n",
      "Assigned bit width: 2, Remaining budget: 1509425152.0\n",
      "Assigned bit width: 2, Remaining budget: 1503789056.0\n",
      "Assigned bit width: 2, Remaining budget: 1498152960.0\n",
      "Assigned bit width: 2, Remaining budget: 1492516864.0\n",
      "Assigned bit width: 2, Remaining budget: 1490419712.0\n",
      "Assigned bit width: 2, Remaining budget: 1488322560.0\n",
      "Assigned bit width: 2, Remaining budget: 1486225408.0\n",
      "Assigned bit width: 2, Remaining budget: 1484128256.0\n",
      "Assigned bit width: 2, Remaining budget: 1478492160.0\n",
      "Assigned bit width: 2, Remaining budget: 1472856064.0\n",
      "Assigned bit width: 2, Remaining budget: 1467219968.0\n",
      "Assigned bit width: 2, Remaining budget: 1465122816.0\n",
      "Assigned bit width: 2, Remaining budget: 1463025664.0\n",
      "Assigned bit width: 2, Remaining budget: 1460928512.0\n",
      "Assigned bit width: 2, Remaining budget: 1458831360.0\n",
      "Assigned bit width: 2, Remaining budget: 1453195264.0\n",
      "Assigned bit width: 2, Remaining budget: 1447559168.0\n",
      "Assigned bit width: 2, Remaining budget: 1441923072.0\n",
      "Assigned bit width: 2, Remaining budget: 1439825920.0\n",
      "Assigned bit width: 2, Remaining budget: 1437728768.0\n",
      "Assigned bit width: 2, Remaining budget: 1435631616.0\n",
      "Assigned bit width: 2, Remaining budget: 1433534464.0\n",
      "Assigned bit width: 2, Remaining budget: 1427898368.0\n",
      "Assigned bit width: 2, Remaining budget: 1422262272.0\n",
      "Assigned bit width: 2, Remaining budget: 1416626176.0\n",
      "Assigned bit width: 2, Remaining budget: 1414529024.0\n",
      "Assigned bit width: 2, Remaining budget: 1412431872.0\n",
      "Assigned bit width: 2, Remaining budget: 1410334720.0\n",
      "Assigned bit width: 2, Remaining budget: 1408237568.0\n",
      "Assigned bit width: 2, Remaining budget: 1402601472.0\n",
      "Assigned bit width: 2, Remaining budget: 1396965376.0\n",
      "Assigned bit width: 2, Remaining budget: 1391329280.0\n",
      "Assigned bit width: 2, Remaining budget: 1389232128.0\n",
      "Assigned bit width: 2, Remaining budget: 1387134976.0\n",
      "Assigned bit width: 2, Remaining budget: 1385037824.0\n",
      "Assigned bit width: 2, Remaining budget: 1382940672.0\n",
      "Assigned bit width: 2, Remaining budget: 1377304576.0\n",
      "Assigned bit width: 2, Remaining budget: 1371668480.0\n",
      "Assigned bit width: 2, Remaining budget: 1366032384.0\n",
      "Assigned bit width: 2, Remaining budget: 1363935232.0\n",
      "Assigned bit width: 2, Remaining budget: 1361838080.0\n",
      "Assigned bit width: 2, Remaining budget: 1359740928.0\n",
      "Assigned bit width: 2, Remaining budget: 1357643776.0\n",
      "Assigned bit width: 2, Remaining budget: 1352007680.0\n",
      "Assigned bit width: 2, Remaining budget: 1346371584.0\n",
      "Assigned bit width: 2, Remaining budget: 1340735488.0\n",
      "Assigned bit width: 2, Remaining budget: 1338638336.0\n",
      "Assigned bit width: 2, Remaining budget: 1336541184.0\n",
      "Assigned bit width: 2, Remaining budget: 1334444032.0\n",
      "Assigned bit width: 2, Remaining budget: 1332346880.0\n",
      "Assigned bit width: 2, Remaining budget: 1326710784.0\n",
      "Assigned bit width: 2, Remaining budget: 1321074688.0\n",
      "Assigned bit width: 2, Remaining budget: 1315438592.0\n",
      "Assigned bit width: 2, Remaining budget: 1313341440.0\n",
      "Assigned bit width: 2, Remaining budget: 1311244288.0\n",
      "Assigned bit width: 2, Remaining budget: 1309147136.0\n",
      "Assigned bit width: 2, Remaining budget: 1307049984.0\n",
      "Assigned bit width: 2, Remaining budget: 1301413888.0\n",
      "Assigned bit width: 2, Remaining budget: 1295777792.0\n",
      "Assigned bit width: 2, Remaining budget: 1290141696.0\n",
      "Assigned bit width: 2, Remaining budget: 1288044544.0\n",
      "Assigned bit width: 2, Remaining budget: 1285947392.0\n",
      "Assigned bit width: 2, Remaining budget: 1283850240.0\n",
      "Assigned bit width: 2, Remaining budget: 1281753088.0\n",
      "Assigned bit width: 2, Remaining budget: 1276116992.0\n",
      "Assigned bit width: 2, Remaining budget: 1270480896.0\n",
      "Assigned bit width: 2, Remaining budget: 1264844800.0\n",
      "Assigned bit width: 2, Remaining budget: 1262747648.0\n",
      "Assigned bit width: 2, Remaining budget: 1260650496.0\n",
      "Assigned bit width: 2, Remaining budget: 1258553344.0\n",
      "Assigned bit width: 2, Remaining budget: 1256456192.0\n",
      "Assigned bit width: 2, Remaining budget: 1250820096.0\n",
      "Assigned bit width: 2, Remaining budget: 1245184000.0\n",
      "Assigned bit width: 2, Remaining budget: 1239547904.0\n",
      "Assigned bit width: 2, Remaining budget: 1237450752.0\n",
      "Assigned bit width: 2, Remaining budget: 1235353600.0\n",
      "Assigned bit width: 2, Remaining budget: 1233256448.0\n",
      "Assigned bit width: 2, Remaining budget: 1231159296.0\n",
      "Assigned bit width: 2, Remaining budget: 1225523200.0\n",
      "Assigned bit width: 2, Remaining budget: 1219887104.0\n",
      "Assigned bit width: 2, Remaining budget: 1214251008.0\n",
      "Assigned bit width: 2, Remaining budget: 1212153856.0\n",
      "Assigned bit width: 2, Remaining budget: 1210056704.0\n",
      "Assigned bit width: 2, Remaining budget: 1207959552.0\n",
      "Assigned bit width: 2, Remaining budget: 1205862400.0\n",
      "Assigned bit width: 2, Remaining budget: 1200226304.0\n",
      "Assigned bit width: 2, Remaining budget: 1194590208.0\n",
      "Assigned bit width: 2, Remaining budget: 1188954112.0\n",
      "Assigned bit width: 2, Remaining budget: 1186856960.0\n",
      "Assigned bit width: 2, Remaining budget: 1184759808.0\n",
      "Assigned bit width: 2, Remaining budget: 1182662656.0\n",
      "Assigned bit width: 2, Remaining budget: 1180565504.0\n",
      "Assigned bit width: 2, Remaining budget: 1174929408.0\n",
      "Assigned bit width: 2, Remaining budget: 1169293312.0\n",
      "Assigned bit width: 2, Remaining budget: 1163657216.0\n",
      "Assigned bit width: 2, Remaining budget: 1161560064.0\n",
      "Assigned bit width: 2, Remaining budget: 1159462912.0\n",
      "Assigned bit width: 2, Remaining budget: 1157365760.0\n",
      "Assigned bit width: 2, Remaining budget: 1155268608.0\n",
      "Assigned bit width: 2, Remaining budget: 1149632512.0\n",
      "Assigned bit width: 2, Remaining budget: 1143996416.0\n",
      "Assigned bit width: 2, Remaining budget: 1138360320.0\n",
      "Assigned bit width: 2, Remaining budget: 1136263168.0\n",
      "Assigned bit width: 2, Remaining budget: 1134166016.0\n",
      "Assigned bit width: 2, Remaining budget: 1132068864.0\n",
      "Assigned bit width: 2, Remaining budget: 1129971712.0\n",
      "Assigned bit width: 2, Remaining budget: 1124335616.0\n",
      "Assigned bit width: 2, Remaining budget: 1118699520.0\n",
      "Assigned bit width: 2, Remaining budget: 1113063424.0\n",
      "Assigned bit width: 2, Remaining budget: 1110966272.0\n",
      "Assigned bit width: 2, Remaining budget: 1108869120.0\n",
      "Assigned bit width: 2, Remaining budget: 1106771968.0\n",
      "Assigned bit width: 2, Remaining budget: 1104674816.0\n",
      "Assigned bit width: 2, Remaining budget: 1099038720.0\n",
      "Assigned bit width: 2, Remaining budget: 1093402624.0\n",
      "Assigned bit width: 2, Remaining budget: 1087766528.0\n",
      "Assigned bit width: 2, Remaining budget: 1085669376.0\n",
      "Assigned bit width: 2, Remaining budget: 1083572224.0\n",
      "Assigned bit width: 2, Remaining budget: 1081475072.0\n",
      "Assigned bit width: 2, Remaining budget: 1079377920.0\n",
      "Assigned bit width: 2, Remaining budget: 1073741824.0\n",
      "Assigned bit width: 2, Remaining budget: 1068105728.0\n",
      "Assigned bit width: 2, Remaining budget: 1062469632.0\n",
      "Assigned bit width: 2, Remaining budget: 1060372480.0\n",
      "Assigned bit width: 2, Remaining budget: 1058275328.0\n",
      "Assigned bit width: 2, Remaining budget: 1056178176.0\n",
      "Assigned bit width: 2, Remaining budget: 1054081024.0\n",
      "Assigned bit width: 2, Remaining budget: 1048444928.0\n",
      "Assigned bit width: 2, Remaining budget: 1042808832.0\n",
      "Assigned bit width: 2, Remaining budget: 1037172736.0\n",
      "Assigned bit width: 2, Remaining budget: 1035075584.0\n",
      "Assigned bit width: 2, Remaining budget: 1032978432.0\n",
      "Assigned bit width: 2, Remaining budget: 1030881280.0\n",
      "Assigned bit width: 2, Remaining budget: 1028784128.0\n",
      "Assigned bit width: 2, Remaining budget: 1023148032.0\n",
      "Assigned bit width: 2, Remaining budget: 1017511936.0\n",
      "Assigned bit width: 2, Remaining budget: 1011875840.0\n",
      "Assigned bit width: 2, Remaining budget: 1009778688.0\n",
      "Assigned bit width: 2, Remaining budget: 1007681536.0\n",
      "Assigned bit width: 2, Remaining budget: 1005584384.0\n",
      "Assigned bit width: 2, Remaining budget: 1003487232.0\n",
      "Assigned bit width: 2, Remaining budget: 997851136.0\n",
      "Assigned bit width: 2, Remaining budget: 992215040.0\n",
      "Assigned bit width: 2, Remaining budget: 986578944.0\n",
      "Assigned bit width: 2, Remaining budget: 984481792.0\n",
      "Assigned bit width: 2, Remaining budget: 982384640.0\n",
      "Assigned bit width: 2, Remaining budget: 980287488.0\n",
      "Assigned bit width: 2, Remaining budget: 978190336.0\n",
      "Assigned bit width: 2, Remaining budget: 972554240.0\n",
      "Assigned bit width: 2, Remaining budget: 966918144.0\n",
      "Assigned bit width: 2, Remaining budget: 961282048.0\n",
      "Assigned bit width: 2, Remaining budget: 959184896.0\n",
      "Assigned bit width: 2, Remaining budget: 957087744.0\n",
      "Assigned bit width: 2, Remaining budget: 954990592.0\n",
      "Assigned bit width: 2, Remaining budget: 952893440.0\n",
      "Assigned bit width: 2, Remaining budget: 947257344.0\n",
      "Assigned bit width: 2, Remaining budget: 941621248.0\n",
      "Assigned bit width: 2, Remaining budget: 935985152.0\n",
      "Assigned bit width: 2, Remaining budget: 933888000.0\n",
      "Assigned bit width: 2, Remaining budget: 931790848.0\n",
      "Assigned bit width: 2, Remaining budget: 929693696.0\n",
      "Assigned bit width: 2, Remaining budget: 927596544.0\n",
      "Assigned bit width: 2, Remaining budget: 921960448.0\n",
      "Assigned bit width: 2, Remaining budget: 916324352.0\n",
      "Assigned bit width: 2, Remaining budget: 910688256.0\n",
      "Assigned bit width: 2, Remaining budget: 908591104.0\n",
      "Assigned bit width: 2, Remaining budget: 906493952.0\n",
      "Assigned bit width: 2, Remaining budget: 904396800.0\n",
      "Assigned bit width: 2, Remaining budget: 902299648.0\n",
      "Assigned bit width: 2, Remaining budget: 896663552.0\n",
      "Assigned bit width: 2, Remaining budget: 891027456.0\n",
      "Assigned bit width: 2, Remaining budget: 885391360.0\n",
      "Assigned bit width: 2, Remaining budget: 883294208.0\n",
      "Assigned bit width: 2, Remaining budget: 881197056.0\n",
      "Assigned bit width: 2, Remaining budget: 879099904.0\n",
      "Assigned bit width: 2, Remaining budget: 877002752.0\n",
      "Assigned bit width: 2, Remaining budget: 871366656.0\n",
      "Assigned bit width: 2, Remaining budget: 865730560.0\n",
      "Assigned bit width: 2, Remaining budget: 860094464.0\n",
      "Assigned bit width: 2, Remaining budget: 857997312.0\n",
      "Assigned bit width: 2, Remaining budget: 855900160.0\n",
      "Assigned bit width: 2, Remaining budget: 853803008.0\n",
      "Assigned bit width: 2, Remaining budget: 851705856.0\n",
      "Assigned bit width: 2, Remaining budget: 846069760.0\n",
      "Assigned bit width: 2, Remaining budget: 840433664.0\n",
      "Assigned bit width: 2, Remaining budget: 834797568.0\n",
      "Assigned bit width: 2, Remaining budget: 832700416.0\n",
      "Assigned bit width: 2, Remaining budget: 830603264.0\n",
      "Assigned bit width: 2, Remaining budget: 828506112.0\n",
      "Assigned bit width: 2, Remaining budget: 826408960.0\n",
      "Assigned bit width: 2, Remaining budget: 820772864.0\n",
      "Assigned bit width: 2, Remaining budget: 815136768.0\n",
      "Assigned bit width: 2, Remaining budget: 809500672.0\n",
      "Final assigned bit widths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    # 加载json文件数据\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    data = []\n",
    "    # 遍历字典中的值并平铺\n",
    "    for id, block in enumerate(json_data):\n",
    "        for key, value in json_data[block].items():\n",
    "            data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "bits = [2, 3, 4, 8]  # 可选位宽\n",
    "F = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/fisher_data.json')\n",
    "F = torch.tensor(F, dtype=torch.float32)\n",
    "P = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/LayersParams.json')\n",
    "p = torch.tensor(P, dtype=torch.float32)\n",
    "N = len(F)  # 层数\n",
    "B = 16  # 原始位宽\n",
    "R = 0.25  # 压缩率\n",
    "alpha = 0.4  # 目标函数中的衰减系数\n",
    "\n",
    "# 初始化RL类\n",
    "rl = RL(bits, F, P, N, B, R, alpha)\n",
    "\n",
    "# 训练\n",
    "rl.train(num_episodes=500)\n",
    "\n",
    "# 测试\n",
    "rl.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810ed24-0d13-4c6f-b526-e42a414cea9a",
   "metadata": {},
   "source": [
    " #  增加模型保存和加载部分内容\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b40db6c-0ed9-48cc-b675-53f4cf76b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# 定义更复杂的 LSTM-based 策略网络\n",
    "class ComplexLSTMPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.2):\n",
    "        super(ComplexLSTMPolicyNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 双向 LSTM 层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers, \n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 全连接层，输出每个可能的位宽的概率\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 双向 LSTM 的输出维度是 hidden_dim * 2\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM 前向传播\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # 取最后一个时间步的输出，并去掉中间的维度\n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        # 应用 Dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 全连接层\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # 使用 softmax 生成概率分布\n",
    "        prob_dist = torch.softmax(out, dim=-1)\n",
    "        \n",
    "        return prob_dist, hidden\n",
    "\n",
    "# 定义强化学习环境\n",
    "class QuantizationEnv:\n",
    "    def __init__(self, layers, fisher_info, bits, p_all, R, alpha, B):\n",
    "        self.layers = layers\n",
    "        self.fisher_info = fisher_info\n",
    "        self.bits = bits\n",
    "        self.p_all = p_all\n",
    "        self.R = R\n",
    "        self.alpha = alpha\n",
    "        self.B = B\n",
    "        self.p_comp = R * p_all\n",
    "        self.current_layer = 0\n",
    "        self.assigned_bits = []\n",
    "        self.remaining_budget = self.p_comp\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_layer = 0\n",
    "        self.assigned_bits = []\n",
    "        self.remaining_budget = self.p_comp\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # 状态包括当前层的索引、已分配的位宽、剩余的参数预算\n",
    "        state = [self.current_layer, self.remaining_budget]\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 获取当前层的位宽\n",
    "        bit_width = self.bits[action]\n",
    "        \n",
    "        # 计算当前层的参数数量\n",
    "        p_i = self.layers[self.current_layer]\n",
    "        \n",
    "        # 更新剩余的参数预算\n",
    "        self.remaining_budget -= p_i * (bit_width / self.B)\n",
    "        \n",
    "        # 记录已分配的位宽\n",
    "        self.assigned_bits.append(bit_width)\n",
    "        \n",
    "        # 计算精度损失\n",
    "        delta_acc = self.fisher_info[self.current_layer] * np.exp(-self.alpha * (self.B / bit_width))\n",
    "        \n",
    "        # 计算 Reward（改进后的 Reward 函数）\n",
    "        reward = -delta_acc + 0.1 * (self.remaining_budget / self.p_comp)  # beta = 0.1\n",
    "        \n",
    "        # 检查是否完成所有层的分配\n",
    "        done = (self.current_layer == len(self.layers) - 1)\n",
    "        \n",
    "        # 更新当前层\n",
    "        self.current_layer += 1\n",
    "        \n",
    "        # 获取下一个状态\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def re_adjust_bits(self):\n",
    "        \"\"\"Re-adjust bit widths to meet the storage constraint\"\"\"\n",
    "        total_storage = sum(p_i * (bit_i / self.B) for p_i, bit_i in zip(self.layers, self.assigned_bits))\n",
    "        \n",
    "        while total_storage > self.p_comp:\n",
    "            # 找到 Fisher 信息最低的层，降低其位宽\n",
    "            min_fisher_idx = np.argmin(self.fisher_info)\n",
    "            if self.assigned_bits[min_fisher_idx] > min(self.bits):\n",
    "                self.assigned_bits[min_fisher_idx] = max(self.assigned_bits[min_fisher_idx] - 1, min(self.bits))\n",
    "            total_storage = sum(p_i * (bit_i / self.B) for p_i, bit_i in zip(self.layers, self.assigned_bits))\n",
    "        \n",
    "        while total_storage < self.p_comp:\n",
    "            # 找到 Fisher 信息最高的层，提高其位宽\n",
    "            max_fisher_idx = np.argmax(self.fisher_info)\n",
    "            if self.assigned_bits[max_fisher_idx] < max(self.bits):\n",
    "                self.assigned_bits[max_fisher_idx] = min(self.assigned_bits[max_fisher_idx] + 1, max(self.bits))\n",
    "            total_storage = sum(p_i * (bit_i / self.B) for p_i, bit_i in zip(self.layers, self.assigned_bits))\n",
    "        \n",
    "        return self.assigned_bits\n",
    "\n",
    "# 定义强化学习类\n",
    "class RL:\n",
    "    def __init__(self, bits, F, P, N, B, R, alpha, input_dim=2, hidden_dim=256, output_dim=None, num_layers=3, lr=1e-3, gamma=0.99, batch_size=64, dropout=0.2):\n",
    "        # 初始化环境参数\n",
    "        self.bits = bits\n",
    "        self.F = F\n",
    "        self.P = P\n",
    "        self.N = N\n",
    "        self.B = B\n",
    "        self.R = R\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # 初始化强化学习超参数\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim if output_dim is not None else len(bits)\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 初始化环境\n",
    "        self.env = QuantizationEnv(self.P, self.F, self.bits, sum(self.P), self.R, self.alpha, self.B)\n",
    "        \n",
    "        # 初始化 Agent\n",
    "        self.agent = self._init_agent()\n",
    "        \n",
    "        # 学习率调度器\n",
    "        self.scheduler = ReduceLROnPlateau(self.agent.optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    def _init_agent(self):\n",
    "        return RLAgent(self.input_dim, self.hidden_dim, self.output_dim, self.num_layers, self.lr, self.gamma, self.dropout)\n",
    "    \n",
    "    def train(self, num_episodes, save_path=\"rl_model.pth\"):\n",
    "        reward_history = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            hidden = None\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # 选择动作\n",
    "                action, hidden = self.agent.select_action(state, hidden)\n",
    "                \n",
    "                # 执行动作\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # 存储转移\n",
    "                self.agent.store_transition(state, action, reward, next_state, done)\n",
    "                \n",
    "                # 更新状态\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                # 训练 Agent\n",
    "                loss = self.agent.train(self.batch_size)\n",
    "                \n",
    "                # 更新学习率\n",
    "                if loss is not None:\n",
    "                    self.scheduler.step(loss)\n",
    "            \n",
    "            # 记录 Reward\n",
    "            reward_history.append(total_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Loss: {loss if loss is not None else 'N/A'}\")\n",
    "        \n",
    "        # 保存模型\n",
    "        self.save_model(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "        return reward_history\n",
    "    \n",
    "    def test(self, load_path=None):\n",
    "        if load_path is not None:\n",
    "            self.load_model(load_path)\n",
    "            print(f\"Model loaded from {load_path}\")\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        hidden = None\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, hidden = self.agent.select_action(state, hidden)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            state = next_state\n",
    "            print(f\"Assigned bit width: {self.bits[action]}, Remaining budget: {self.env.remaining_budget}\")\n",
    "        \n",
    "        # Re-adjust bit widths\n",
    "        final_bits = self.env.re_adjust_bits()\n",
    "        print(\"Final assigned bit widths after re-adjustment:\", final_bits)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"保存模型和优化器状态\"\"\"\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.agent.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.agent.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"加载模型和优化器状态\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"Model and optimizer loaded successfully.\")\n",
    "\n",
    "# 定义强化学习 Agent\n",
    "class RLAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, lr=1e-3, gamma=0.99, dropout=0.2):\n",
    "        self.policy_net = ComplexLSTMPolicyNetwork(input_dim, hidden_dim, output_dim, num_layers, dropout)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # 用于存储经验回放\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "    def select_action(self, state, hidden):\n",
    "        # 将状态转换为 Tensor，并增加一个时间步维度\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)  # [1, 1, input_dim]\n",
    "        \n",
    "        # 通过策略网络生成动作概率分布\n",
    "        prob_dist, hidden = self.policy_net(state, hidden)\n",
    "        \n",
    "        # 根据概率分布采样动作\n",
    "        action = torch.multinomial(prob_dist, 1).item()\n",
    "        \n",
    "        return action, hidden\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # 从经验回放中随机采样一个 batch\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states).unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # 计算当前状态的 Q 值\n",
    "        current_q_values, _ = self.policy_net(states, None)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # 计算下一个状态的 Q 值\n",
    "        next_q_values, _ = self.policy_net(next_states, None)\n",
    "        next_q_values = next_q_values.max(1)[0].detach()\n",
    "        \n",
    "        # 计算目标 Q 值\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62ecad-27fc-40c7-9f01-b3e0d291c0c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00049: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 00088: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 00102: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 00124: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 00144: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 00155: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Episode 1, Total Reward: -691.9647827148438, Loss: 758.8317260742188\n",
      "Epoch 00166: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 00177: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 00188: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 00199: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 00210: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 00221: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 00232: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Episode 2, Total Reward: -160.8243865966797, Loss: 753.9638061523438\n",
      "Episode 3, Total Reward: -160.8243865966797, Loss: 755.1854248046875\n",
      "Episode 4, Total Reward: -160.8243865966797, Loss: 2.185300350189209\n",
      "Episode 5, Total Reward: -160.8243865966797, Loss: 0.9263758063316345\n",
      "Episode 6, Total Reward: -160.8243865966797, Loss: 754.3928833007812\n",
      "Episode 7, Total Reward: -160.8243865966797, Loss: 3.4331881999969482\n",
      "Episode 8, Total Reward: -160.8243865966797, Loss: 1.966468095779419\n",
      "Episode 9, Total Reward: -160.8243865966797, Loss: 0.5961970686912537\n",
      "Episode 10, Total Reward: -160.8243865966797, Loss: 0.4809538722038269\n",
      "Episode 11, Total Reward: -160.8243865966797, Loss: 1.3512077331542969\n",
      "Episode 12, Total Reward: -160.8243865966797, Loss: 1.213782548904419\n",
      "Episode 13, Total Reward: -160.8243865966797, Loss: 6.637091159820557\n",
      "Episode 14, Total Reward: -160.8243865966797, Loss: 1.0978409051895142\n",
      "Episode 15, Total Reward: -160.8243865966797, Loss: 191.63357543945312\n",
      "Episode 16, Total Reward: -160.8243865966797, Loss: 2.047222137451172\n",
      "Episode 17, Total Reward: -160.8243865966797, Loss: 6.648017883300781\n",
      "Episode 18, Total Reward: -160.8243865966797, Loss: 0.46827226877212524\n",
      "Episode 19, Total Reward: -160.8243865966797, Loss: 2.8446693420410156\n",
      "Episode 20, Total Reward: -160.8243865966797, Loss: 6.769388198852539\n",
      "Episode 21, Total Reward: -160.8243865966797, Loss: 0.4316818416118622\n",
      "Episode 22, Total Reward: -160.8243865966797, Loss: 2.323857307434082\n",
      "Episode 23, Total Reward: -160.8243865966797, Loss: 14.782685279846191\n",
      "Episode 24, Total Reward: -160.8243865966797, Loss: 2.1669230461120605\n",
      "Episode 25, Total Reward: -160.8243865966797, Loss: 0.3970244824886322\n",
      "Episode 26, Total Reward: -160.8243865966797, Loss: 1.631085753440857\n",
      "Episode 27, Total Reward: -160.8243865966797, Loss: 0.42374375462532043\n",
      "Episode 28, Total Reward: -160.8243865966797, Loss: 0.34899288415908813\n",
      "Episode 29, Total Reward: -160.8243865966797, Loss: 0.36967942118644714\n",
      "Episode 30, Total Reward: -160.8243865966797, Loss: 0.4193536043167114\n",
      "Episode 31, Total Reward: -160.8243865966797, Loss: 0.3861919641494751\n",
      "Episode 32, Total Reward: -160.8243865966797, Loss: 1.7682712078094482\n",
      "Episode 33, Total Reward: -160.8243865966797, Loss: 0.7074685096740723\n",
      "Episode 34, Total Reward: -160.8243865966797, Loss: 0.6796554327011108\n",
      "Episode 35, Total Reward: -160.8243865966797, Loss: 0.4152924716472626\n",
      "Episode 36, Total Reward: -160.8243865966797, Loss: 0.5224432945251465\n",
      "Episode 37, Total Reward: -160.8243865966797, Loss: 0.4446537494659424\n",
      "Episode 38, Total Reward: -160.8243865966797, Loss: 6.653911590576172\n",
      "Episode 39, Total Reward: -160.8243865966797, Loss: 0.4172772467136383\n",
      "Episode 40, Total Reward: -160.8243865966797, Loss: 0.6084641814231873\n",
      "Episode 41, Total Reward: -160.8243865966797, Loss: 6.632544040679932\n",
      "Episode 42, Total Reward: -160.8243865966797, Loss: 0.3587547242641449\n",
      "Episode 43, Total Reward: -160.8243865966797, Loss: 2.052708148956299\n",
      "Episode 44, Total Reward: -160.8243865966797, Loss: 0.4201590120792389\n",
      "Episode 45, Total Reward: -160.8243865966797, Loss: 0.5934487581253052\n",
      "Episode 46, Total Reward: -160.8243865966797, Loss: 0.42749643325805664\n",
      "Episode 47, Total Reward: -160.8243865966797, Loss: 6.876974582672119\n",
      "Episode 48, Total Reward: -160.8243865966797, Loss: 0.39436009526252747\n",
      "Episode 49, Total Reward: -160.8243865966797, Loss: 0.3535356819629669\n",
      "Episode 50, Total Reward: -160.8243865966797, Loss: 8.093693733215332\n",
      "Episode 51, Total Reward: -160.8243865966797, Loss: 0.3637915551662445\n",
      "Episode 52, Total Reward: -160.8243865966797, Loss: 0.3212541937828064\n",
      "Episode 53, Total Reward: -160.8243865966797, Loss: 0.38156771659851074\n",
      "Episode 54, Total Reward: -160.8243865966797, Loss: 0.4149843752384186\n",
      "Episode 55, Total Reward: -160.8243865966797, Loss: 0.4108203649520874\n",
      "Episode 56, Total Reward: -160.8243865966797, Loss: 0.4923260509967804\n",
      "Episode 57, Total Reward: -160.8243865966797, Loss: 1.9944955110549927\n",
      "Episode 58, Total Reward: -160.8243865966797, Loss: 6.827480792999268\n",
      "Episode 59, Total Reward: -160.8243865966797, Loss: 6.6415252685546875\n",
      "Episode 60, Total Reward: -160.8243865966797, Loss: 0.350380003452301\n",
      "Episode 61, Total Reward: -160.8243865966797, Loss: 12.743132591247559\n",
      "Episode 62, Total Reward: -160.8243865966797, Loss: 0.7008702754974365\n",
      "Episode 63, Total Reward: -160.8243865966797, Loss: 0.3260510563850403\n",
      "Episode 64, Total Reward: -160.8243865966797, Loss: 6.538802146911621\n",
      "Episode 65, Total Reward: -160.8243865966797, Loss: 0.44769054651260376\n",
      "Episode 66, Total Reward: -160.8243865966797, Loss: 0.6273800730705261\n",
      "Episode 67, Total Reward: -160.8243865966797, Loss: 0.41462570428848267\n",
      "Episode 68, Total Reward: -160.8243865966797, Loss: 0.6897291541099548\n",
      "Episode 69, Total Reward: -160.8243865966797, Loss: 6.5710062980651855\n",
      "Episode 70, Total Reward: -160.8243865966797, Loss: 6.6134843826293945\n",
      "Episode 71, Total Reward: -160.8243865966797, Loss: 1.948944330215454\n",
      "Episode 72, Total Reward: -160.8243865966797, Loss: 0.5875974893569946\n",
      "Episode 73, Total Reward: -160.8243865966797, Loss: 0.561987578868866\n",
      "Episode 74, Total Reward: -160.8243865966797, Loss: 0.5731114149093628\n",
      "Episode 75, Total Reward: -160.8243865966797, Loss: 0.8771918416023254\n",
      "Episode 76, Total Reward: -160.8243865966797, Loss: 0.4360303282737732\n",
      "Episode 77, Total Reward: -160.8243865966797, Loss: 2.2265307903289795\n",
      "Episode 78, Total Reward: -160.8243865966797, Loss: 0.3327762484550476\n",
      "Episode 79, Total Reward: -160.8243865966797, Loss: 0.40296670794487\n",
      "Episode 80, Total Reward: -160.8243865966797, Loss: 1.964577078819275\n",
      "Episode 81, Total Reward: -160.8243865966797, Loss: 0.3598378002643585\n",
      "Episode 82, Total Reward: -160.8243865966797, Loss: 6.615030288696289\n",
      "Episode 83, Total Reward: -160.8243865966797, Loss: 6.579404830932617\n",
      "Episode 84, Total Reward: -160.8243865966797, Loss: 0.6579834222793579\n",
      "Episode 85, Total Reward: -160.8243865966797, Loss: 0.3879774212837219\n",
      "Episode 86, Total Reward: -160.8243865966797, Loss: 0.40659958124160767\n",
      "Episode 87, Total Reward: -160.8243865966797, Loss: 0.4268144965171814\n",
      "Episode 88, Total Reward: -160.8243865966797, Loss: 0.6198348999023438\n",
      "Episode 89, Total Reward: -160.8243865966797, Loss: 0.40654030442237854\n",
      "Episode 90, Total Reward: -160.8243865966797, Loss: 6.643959045410156\n",
      "Episode 91, Total Reward: -160.8243865966797, Loss: 0.3352741301059723\n",
      "Episode 92, Total Reward: -160.8243865966797, Loss: 12.720407485961914\n",
      "Episode 93, Total Reward: -160.8243865966797, Loss: 1.9471025466918945\n",
      "Episode 94, Total Reward: -160.8243865966797, Loss: 0.4184879660606384\n",
      "Episode 95, Total Reward: -160.8243865966797, Loss: 1.9564223289489746\n",
      "Episode 96, Total Reward: -160.8243865966797, Loss: 0.4511363208293915\n",
      "Episode 97, Total Reward: -160.8243865966797, Loss: 6.622493267059326\n",
      "Episode 98, Total Reward: -160.8243865966797, Loss: 0.3858107626438141\n",
      "Episode 99, Total Reward: -160.8243865966797, Loss: 0.8043882846832275\n",
      "Episode 100, Total Reward: -160.8243865966797, Loss: 0.3489140272140503\n",
      "Episode 101, Total Reward: -160.8243865966797, Loss: 0.683975338935852\n",
      "Episode 102, Total Reward: -160.8243865966797, Loss: 6.598110198974609\n",
      "Episode 103, Total Reward: -160.8243865966797, Loss: 0.6723108887672424\n",
      "Episode 104, Total Reward: -160.8243865966797, Loss: 0.610388994216919\n",
      "Episode 105, Total Reward: -160.8243865966797, Loss: 6.614631652832031\n",
      "Episode 106, Total Reward: -160.8243865966797, Loss: 2.1853339672088623\n",
      "Episode 107, Total Reward: -160.8243865966797, Loss: 2.175034523010254\n",
      "Episode 108, Total Reward: -160.8243865966797, Loss: 6.559110164642334\n",
      "Episode 109, Total Reward: -160.8243865966797, Loss: 0.3840411305427551\n",
      "Episode 110, Total Reward: -160.8243865966797, Loss: 0.36795228719711304\n",
      "Episode 111, Total Reward: -160.8243865966797, Loss: 0.3498534560203552\n",
      "Episode 112, Total Reward: -160.8243865966797, Loss: 3.491128444671631\n",
      "Episode 113, Total Reward: -160.8243865966797, Loss: 0.34153515100479126\n",
      "Episode 114, Total Reward: -160.8243865966797, Loss: 8.162487030029297\n",
      "Episode 115, Total Reward: -160.8243865966797, Loss: 1.915998101234436\n",
      "Episode 116, Total Reward: -160.8243865966797, Loss: 1.9847362041473389\n",
      "Episode 117, Total Reward: -160.8243865966797, Loss: 6.573300361633301\n",
      "Episode 118, Total Reward: -160.8243865966797, Loss: 0.35453158617019653\n",
      "Episode 119, Total Reward: -160.8243865966797, Loss: 0.421958863735199\n",
      "Episode 120, Total Reward: -160.8243865966797, Loss: 0.37432995438575745\n",
      "Episode 121, Total Reward: -160.8243865966797, Loss: 0.5910474061965942\n",
      "Episode 122, Total Reward: -162.28890991210938, Loss: 0.31797724962234497\n",
      "Episode 123, Total Reward: -160.8243865966797, Loss: 6.620085716247559\n",
      "Episode 124, Total Reward: -160.8243865966797, Loss: 0.7049083709716797\n",
      "Episode 125, Total Reward: -160.8243865966797, Loss: 2.233663558959961\n",
      "Episode 126, Total Reward: -160.8243865966797, Loss: 0.39960983395576477\n",
      "Episode 127, Total Reward: -160.8243865966797, Loss: 0.42120349407196045\n",
      "Episode 128, Total Reward: -160.8243865966797, Loss: 0.3807830214500427\n",
      "Episode 129, Total Reward: -160.8243865966797, Loss: 0.39587950706481934\n",
      "Episode 130, Total Reward: -160.8243865966797, Loss: 2.1903913021087646\n",
      "Episode 131, Total Reward: -160.8243865966797, Loss: 0.4026276469230652\n",
      "Episode 132, Total Reward: -160.8243865966797, Loss: 0.4389241635799408\n",
      "Episode 133, Total Reward: -160.8243865966797, Loss: 0.39079487323760986\n",
      "Episode 134, Total Reward: -160.8243865966797, Loss: 0.3554743826389313\n",
      "Episode 135, Total Reward: -160.8243865966797, Loss: 0.7212263941764832\n",
      "Episode 136, Total Reward: -160.8243865966797, Loss: 0.44007816910743713\n",
      "Episode 137, Total Reward: -160.8243865966797, Loss: 0.3471301794052124\n",
      "Episode 138, Total Reward: -160.8243865966797, Loss: 0.34713566303253174\n",
      "Episode 139, Total Reward: -160.8243865966797, Loss: 6.574522972106934\n",
      "Episode 140, Total Reward: -160.8243865966797, Loss: 6.541839122772217\n",
      "Episode 141, Total Reward: -160.8243865966797, Loss: 0.3478256165981293\n",
      "Episode 142, Total Reward: -160.8243865966797, Loss: 1.9099323749542236\n",
      "Episode 143, Total Reward: -160.8243865966797, Loss: 0.6460760831832886\n",
      "Episode 144, Total Reward: -160.8243865966797, Loss: 0.4866420030593872\n",
      "Episode 145, Total Reward: -160.8243865966797, Loss: 6.814550399780273\n",
      "Episode 146, Total Reward: -160.8243865966797, Loss: 1.9087390899658203\n",
      "Episode 147, Total Reward: -160.8243865966797, Loss: 0.4279303550720215\n",
      "Episode 148, Total Reward: -160.8243865966797, Loss: 1.871980905532837\n",
      "Episode 149, Total Reward: -160.8243865966797, Loss: 6.612008094787598\n",
      "Episode 150, Total Reward: -160.8243865966797, Loss: 8.511770248413086\n",
      "Episode 151, Total Reward: -160.8243865966797, Loss: 6.555418968200684\n",
      "Episode 152, Total Reward: -160.8243865966797, Loss: 0.3734344244003296\n",
      "Episode 153, Total Reward: -160.8243865966797, Loss: 0.4690926969051361\n",
      "Episode 154, Total Reward: -160.8243865966797, Loss: 0.6141725778579712\n",
      "Episode 155, Total Reward: -160.8243865966797, Loss: 0.5953304171562195\n",
      "Episode 156, Total Reward: -160.8243865966797, Loss: 1.9426562786102295\n",
      "Episode 157, Total Reward: -160.8243865966797, Loss: 0.6454224586486816\n",
      "Episode 158, Total Reward: -160.8243865966797, Loss: 0.3643426299095154\n",
      "Episode 159, Total Reward: -160.8243865966797, Loss: 1.9873782396316528\n",
      "Episode 160, Total Reward: -160.8243865966797, Loss: 0.41021913290023804\n",
      "Episode 161, Total Reward: -160.8243865966797, Loss: 0.5015261173248291\n",
      "Episode 162, Total Reward: -160.8243865966797, Loss: 0.5570986270904541\n",
      "Episode 163, Total Reward: -160.8243865966797, Loss: 0.48768350481987\n",
      "Episode 164, Total Reward: -160.8243865966797, Loss: 12.757722854614258\n",
      "Episode 165, Total Reward: -160.8243865966797, Loss: 8.395072937011719\n",
      "Episode 166, Total Reward: -160.8243865966797, Loss: 0.6234352588653564\n",
      "Episode 167, Total Reward: -160.8243865966797, Loss: 1.8926362991333008\n",
      "Episode 168, Total Reward: -160.8243865966797, Loss: 3.522277355194092\n",
      "Episode 169, Total Reward: -160.8243865966797, Loss: 0.4184972047805786\n",
      "Episode 170, Total Reward: -160.8243865966797, Loss: 0.39329689741134644\n",
      "Episode 171, Total Reward: -160.8243865966797, Loss: 0.32567423582077026\n",
      "Episode 172, Total Reward: -160.8243865966797, Loss: 0.6275085806846619\n",
      "Episode 173, Total Reward: -160.8243865966797, Loss: 2.0072481632232666\n",
      "Episode 174, Total Reward: -160.8243865966797, Loss: 0.6417421698570251\n",
      "Episode 175, Total Reward: -160.8243865966797, Loss: 0.3982522487640381\n",
      "Episode 176, Total Reward: -160.8243865966797, Loss: 0.3758356273174286\n",
      "Episode 177, Total Reward: -160.8243865966797, Loss: 0.4010127782821655\n",
      "Episode 178, Total Reward: -160.8243865966797, Loss: 2.2580292224884033\n",
      "Episode 179, Total Reward: -160.8243865966797, Loss: 7.197254180908203\n",
      "Episode 180, Total Reward: -160.8243865966797, Loss: 2.17987322807312\n",
      "Episode 181, Total Reward: -160.8243865966797, Loss: 1.9184291362762451\n",
      "Episode 182, Total Reward: -160.8243865966797, Loss: 2.014193058013916\n",
      "Episode 183, Total Reward: -160.8243865966797, Loss: 0.6011620163917542\n",
      "Episode 184, Total Reward: -160.8243865966797, Loss: 0.42914247512817383\n",
      "Episode 185, Total Reward: -160.8243865966797, Loss: 0.33171671628952026\n",
      "Episode 186, Total Reward: -160.8243865966797, Loss: 8.47715950012207\n",
      "Episode 187, Total Reward: -160.8243865966797, Loss: 0.38328710198402405\n",
      "Episode 188, Total Reward: -160.8243865966797, Loss: 8.1981201171875\n",
      "Episode 189, Total Reward: -160.8243865966797, Loss: 6.605798721313477\n",
      "Episode 190, Total Reward: -160.8243865966797, Loss: 2.220097303390503\n",
      "Episode 191, Total Reward: -160.8243865966797, Loss: 0.580417275428772\n",
      "Episode 192, Total Reward: -160.8243865966797, Loss: 6.7133660316467285\n",
      "Episode 193, Total Reward: -160.8243865966797, Loss: 0.3899310827255249\n",
      "Episode 194, Total Reward: -160.8243865966797, Loss: 12.777034759521484\n",
      "Episode 195, Total Reward: -160.8243865966797, Loss: 8.192461967468262\n",
      "Episode 196, Total Reward: -160.8243865966797, Loss: 0.34787508845329285\n",
      "Episode 197, Total Reward: -160.8243865966797, Loss: 0.6472468972206116\n",
      "Episode 198, Total Reward: -160.8243865966797, Loss: 6.6266889572143555\n",
      "Episode 199, Total Reward: -160.8243865966797, Loss: 0.377804160118103\n",
      "Episode 200, Total Reward: -160.8243865966797, Loss: 0.39643535017967224\n",
      "Episode 201, Total Reward: -160.8243865966797, Loss: 0.40445974469184875\n",
      "Episode 202, Total Reward: -160.8243865966797, Loss: 0.48917093873023987\n",
      "Episode 203, Total Reward: -160.8243865966797, Loss: 0.5574350357055664\n",
      "Episode 204, Total Reward: -160.8243865966797, Loss: 1.9110093116760254\n",
      "Episode 205, Total Reward: -160.8243865966797, Loss: 0.3313467502593994\n",
      "Episode 206, Total Reward: -160.8243865966797, Loss: 0.3684612512588501\n",
      "Episode 207, Total Reward: -160.8243865966797, Loss: 13.053701400756836\n",
      "Episode 208, Total Reward: -160.8243865966797, Loss: 0.3124496638774872\n",
      "Episode 209, Total Reward: -160.8243865966797, Loss: 6.638644695281982\n",
      "Episode 210, Total Reward: -160.8243865966797, Loss: 6.658693313598633\n",
      "Episode 211, Total Reward: -160.8243865966797, Loss: 2.1805922985076904\n",
      "Episode 212, Total Reward: -160.8243865966797, Loss: 6.647104263305664\n",
      "Episode 213, Total Reward: -160.8243865966797, Loss: 0.34932732582092285\n",
      "Episode 214, Total Reward: -160.8243865966797, Loss: 2.0273373126983643\n",
      "Episode 215, Total Reward: -160.8243865966797, Loss: 8.212665557861328\n",
      "Episode 216, Total Reward: -160.8243865966797, Loss: 0.3688138723373413\n",
      "Episode 217, Total Reward: -160.8243865966797, Loss: 0.3425038456916809\n",
      "Episode 218, Total Reward: -160.8243865966797, Loss: 3.5444223880767822\n",
      "Episode 219, Total Reward: -160.8243865966797, Loss: 0.3607892394065857\n",
      "Episode 220, Total Reward: -160.8243865966797, Loss: 0.34330374002456665\n",
      "Episode 221, Total Reward: -160.8243865966797, Loss: 6.58608341217041\n",
      "Episode 222, Total Reward: -160.8243865966797, Loss: 6.5886969566345215\n",
      "Episode 223, Total Reward: -160.8243865966797, Loss: 1.9626755714416504\n",
      "Episode 224, Total Reward: -160.8243865966797, Loss: 2.0205671787261963\n",
      "Episode 225, Total Reward: -160.8243865966797, Loss: 0.4237964451313019\n",
      "Episode 226, Total Reward: -160.8243865966797, Loss: 0.7857411503791809\n",
      "Episode 227, Total Reward: -160.8243865966797, Loss: 0.3933194875717163\n",
      "Episode 228, Total Reward: -160.8243865966797, Loss: 0.3496769368648529\n",
      "Episode 229, Total Reward: -160.8243865966797, Loss: 0.34653329849243164\n",
      "Episode 230, Total Reward: -160.8243865966797, Loss: 0.3319915235042572\n",
      "Episode 231, Total Reward: -160.8243865966797, Loss: 0.4008027911186218\n",
      "Episode 232, Total Reward: -160.8243865966797, Loss: 1.9706084728240967\n",
      "Episode 233, Total Reward: -160.8243865966797, Loss: 1.999927282333374\n",
      "Episode 234, Total Reward: -160.8243865966797, Loss: 1.9554197788238525\n",
      "Episode 235, Total Reward: -160.8243865966797, Loss: 0.3693556785583496\n",
      "Episode 236, Total Reward: -160.8243865966797, Loss: 8.15917682647705\n",
      "Episode 237, Total Reward: -160.8243865966797, Loss: 0.3048483431339264\n",
      "Episode 238, Total Reward: -160.8243865966797, Loss: 0.38390684127807617\n",
      "Episode 239, Total Reward: -160.8243865966797, Loss: 0.6568944454193115\n",
      "Episode 240, Total Reward: -160.8243865966797, Loss: 0.46709561347961426\n",
      "Episode 241, Total Reward: -160.8243865966797, Loss: 0.3747328817844391\n",
      "Episode 242, Total Reward: -160.8243865966797, Loss: 8.195218086242676\n",
      "Episode 243, Total Reward: -160.8243865966797, Loss: 7.029147624969482\n",
      "Episode 244, Total Reward: -160.8243865966797, Loss: 6.555590629577637\n",
      "Episode 245, Total Reward: -160.8243865966797, Loss: 0.37584540247917175\n",
      "Episode 246, Total Reward: -160.8243865966797, Loss: 0.3760538101196289\n",
      "Episode 247, Total Reward: -160.8243865966797, Loss: 0.35207438468933105\n",
      "Episode 248, Total Reward: -160.8243865966797, Loss: 0.3896408975124359\n",
      "Episode 249, Total Reward: -160.8243865966797, Loss: 0.37583962082862854\n",
      "Episode 250, Total Reward: -160.8243865966797, Loss: 0.36288976669311523\n",
      "Episode 251, Total Reward: -160.8243865966797, Loss: 6.557229518890381\n",
      "Episode 252, Total Reward: -160.8243865966797, Loss: 0.6296154260635376\n",
      "Episode 253, Total Reward: -160.8243865966797, Loss: 6.579290390014648\n",
      "Episode 254, Total Reward: -160.8243865966797, Loss: 1.9343228340148926\n",
      "Episode 255, Total Reward: -160.8243865966797, Loss: 1.9692389965057373\n",
      "Episode 256, Total Reward: -160.8243865966797, Loss: 6.6223835945129395\n",
      "Episode 257, Total Reward: -160.8243865966797, Loss: 0.3660122752189636\n",
      "Episode 258, Total Reward: -160.8243865966797, Loss: 0.8284782767295837\n",
      "Episode 259, Total Reward: -160.8243865966797, Loss: 0.6098747253417969\n",
      "Episode 260, Total Reward: -160.8243865966797, Loss: 0.5897812843322754\n",
      "Episode 261, Total Reward: -160.8243865966797, Loss: 2.1839253902435303\n",
      "Episode 262, Total Reward: -160.8243865966797, Loss: 0.407299280166626\n",
      "Episode 263, Total Reward: -160.8243865966797, Loss: 0.514519214630127\n",
      "Episode 264, Total Reward: -160.8243865966797, Loss: 0.42038673162460327\n",
      "Episode 265, Total Reward: -160.8243865966797, Loss: 0.6010041236877441\n",
      "Episode 266, Total Reward: -160.8243865966797, Loss: 0.5393850207328796\n",
      "Episode 267, Total Reward: -160.8243865966797, Loss: 1.9800894260406494\n",
      "Episode 268, Total Reward: -160.8243865966797, Loss: 0.6399581432342529\n",
      "Episode 269, Total Reward: -160.8243865966797, Loss: 1.9160274267196655\n",
      "Episode 270, Total Reward: -160.8243865966797, Loss: 0.36551108956336975\n",
      "Episode 271, Total Reward: -160.8243865966797, Loss: 0.6715154051780701\n",
      "Episode 272, Total Reward: -160.8243865966797, Loss: 0.7356172800064087\n",
      "Episode 273, Total Reward: -160.8243865966797, Loss: 8.152047157287598\n",
      "Episode 274, Total Reward: -160.8243865966797, Loss: 0.6243801712989807\n",
      "Episode 275, Total Reward: -160.8243865966797, Loss: 0.3590852618217468\n",
      "Episode 276, Total Reward: -160.8243865966797, Loss: 0.32739174365997314\n",
      "Episode 277, Total Reward: -160.8243865966797, Loss: 6.580735206604004\n",
      "Episode 278, Total Reward: -160.8243865966797, Loss: 0.3922886252403259\n",
      "Episode 279, Total Reward: -160.8243865966797, Loss: 6.587706089019775\n",
      "Episode 280, Total Reward: -160.8243865966797, Loss: 0.40379956364631653\n",
      "Episode 281, Total Reward: -160.8243865966797, Loss: 0.34513184428215027\n",
      "Episode 282, Total Reward: -160.8243865966797, Loss: 0.37955111265182495\n",
      "Episode 283, Total Reward: -160.8243865966797, Loss: 6.556830883026123\n",
      "Episode 284, Total Reward: -162.0194854736328, Loss: 8.616171836853027\n",
      "Episode 285, Total Reward: -160.8243865966797, Loss: 1.9104338884353638\n",
      "Episode 286, Total Reward: -160.8243865966797, Loss: 0.7967541813850403\n",
      "Episode 287, Total Reward: -160.8243865966797, Loss: 0.33055397868156433\n",
      "Episode 288, Total Reward: -161.91632080078125, Loss: 0.5981708765029907\n",
      "Episode 289, Total Reward: -160.8243865966797, Loss: 16.373998641967773\n",
      "Episode 290, Total Reward: -160.8243865966797, Loss: 0.6172493696212769\n",
      "Episode 291, Total Reward: -160.8243865966797, Loss: 8.113237380981445\n",
      "Episode 292, Total Reward: -160.8243865966797, Loss: 2.0453011989593506\n",
      "Episode 293, Total Reward: -160.8243865966797, Loss: 0.2880690097808838\n",
      "Episode 294, Total Reward: -160.8243865966797, Loss: 1.9412100315093994\n",
      "Episode 295, Total Reward: -160.8243865966797, Loss: 6.555810451507568\n",
      "Episode 296, Total Reward: -160.8243865966797, Loss: 0.328860878944397\n",
      "Episode 297, Total Reward: -160.8243865966797, Loss: 8.119644165039062\n",
      "Episode 298, Total Reward: -160.8243865966797, Loss: 0.3728748857975006\n",
      "Episode 299, Total Reward: -160.8243865966797, Loss: 6.5520830154418945\n",
      "Episode 300, Total Reward: -160.8243865966797, Loss: 0.35139042139053345\n",
      "Episode 301, Total Reward: -160.8243865966797, Loss: 3.7283129692077637\n",
      "Episode 302, Total Reward: -160.8243865966797, Loss: 14.65572452545166\n",
      "Episode 303, Total Reward: -160.8243865966797, Loss: 0.3752326965332031\n",
      "Episode 304, Total Reward: -160.8243865966797, Loss: 1.9155139923095703\n",
      "Episode 305, Total Reward: -160.8243865966797, Loss: 0.38310927152633667\n",
      "Episode 306, Total Reward: -160.8243865966797, Loss: 0.3550351560115814\n",
      "Episode 307, Total Reward: -160.8243865966797, Loss: 0.6456043720245361\n",
      "Episode 308, Total Reward: -160.8243865966797, Loss: 1.8859119415283203\n",
      "Episode 309, Total Reward: -160.8243865966797, Loss: 0.7254528999328613\n",
      "Episode 310, Total Reward: -160.8243865966797, Loss: 3.5015082359313965\n",
      "Episode 311, Total Reward: -160.8243865966797, Loss: 0.40741339325904846\n",
      "Episode 312, Total Reward: -160.8243865966797, Loss: 1.860573649406433\n",
      "Episode 313, Total Reward: -160.8243865966797, Loss: 0.43029382824897766\n",
      "Episode 314, Total Reward: -160.8243865966797, Loss: 0.3998768925666809\n",
      "Episode 315, Total Reward: -160.8243865966797, Loss: 6.580578327178955\n",
      "Episode 316, Total Reward: -160.8243865966797, Loss: 0.34345680475234985\n",
      "Episode 317, Total Reward: -160.8243865966797, Loss: 9.785130500793457\n",
      "Episode 318, Total Reward: -160.8243865966797, Loss: 6.953310489654541\n",
      "Episode 319, Total Reward: -160.8243865966797, Loss: 5.090038776397705\n",
      "Episode 320, Total Reward: -160.8243865966797, Loss: 12.825695037841797\n",
      "Episode 321, Total Reward: -160.8243865966797, Loss: 0.42213544249534607\n",
      "Episode 322, Total Reward: -160.8243865966797, Loss: 0.38189560174942017\n",
      "Episode 323, Total Reward: -160.8243865966797, Loss: 0.43889570236206055\n",
      "Episode 324, Total Reward: -160.8243865966797, Loss: 6.596863746643066\n",
      "Episode 325, Total Reward: -160.8243865966797, Loss: 0.35618290305137634\n",
      "Episode 326, Total Reward: -160.8243865966797, Loss: 8.15336799621582\n",
      "Episode 327, Total Reward: -160.8243865966797, Loss: 3.709516763687134\n",
      "Episode 328, Total Reward: -160.8243865966797, Loss: 3.529233694076538\n",
      "Episode 329, Total Reward: -160.8243865966797, Loss: 0.35492631793022156\n",
      "Episode 330, Total Reward: -160.8243865966797, Loss: 6.566500663757324\n",
      "Episode 331, Total Reward: -160.8243865966797, Loss: 0.38552942872047424\n",
      "Episode 332, Total Reward: -160.8243865966797, Loss: 1.99576735496521\n",
      "Episode 333, Total Reward: -160.8243865966797, Loss: 0.8322519659996033\n",
      "Episode 334, Total Reward: -160.8243865966797, Loss: 6.561863422393799\n",
      "Episode 335, Total Reward: -160.8243865966797, Loss: 0.358882337808609\n",
      "Episode 336, Total Reward: -160.8243865966797, Loss: 0.47995322942733765\n",
      "Episode 337, Total Reward: -160.8243865966797, Loss: 0.36621490120887756\n",
      "Episode 338, Total Reward: -160.8243865966797, Loss: 0.37555694580078125\n",
      "Episode 339, Total Reward: -160.8243865966797, Loss: 0.3233139216899872\n",
      "Episode 340, Total Reward: -160.8243865966797, Loss: 13.00439453125\n",
      "Episode 341, Total Reward: -160.8243865966797, Loss: 0.5023485422134399\n",
      "Episode 342, Total Reward: -160.8243865966797, Loss: 6.617208957672119\n",
      "Episode 343, Total Reward: -160.8243865966797, Loss: 1.944138765335083\n",
      "Episode 344, Total Reward: -160.8243865966797, Loss: 0.44960683584213257\n",
      "Episode 345, Total Reward: -160.8243865966797, Loss: 0.34050580859184265\n",
      "Episode 346, Total Reward: -160.8243865966797, Loss: 0.3864310085773468\n",
      "Episode 347, Total Reward: -160.8243865966797, Loss: 1.9051940441131592\n",
      "Episode 348, Total Reward: -160.8243865966797, Loss: 0.3408246636390686\n",
      "Episode 349, Total Reward: -160.8243865966797, Loss: 12.820019721984863\n",
      "Episode 350, Total Reward: -162.0038299560547, Loss: 12.795845031738281\n",
      "Episode 351, Total Reward: -160.8243865966797, Loss: 0.5410985350608826\n",
      "Episode 352, Total Reward: -160.8243865966797, Loss: 6.583295822143555\n",
      "Episode 353, Total Reward: -160.8243865966797, Loss: 9.9802827835083\n",
      "Episode 354, Total Reward: -160.8243865966797, Loss: 0.6891086101531982\n",
      "Episode 355, Total Reward: -160.8243865966797, Loss: 6.607356548309326\n",
      "Episode 356, Total Reward: -160.8243865966797, Loss: 0.5230712890625\n",
      "Episode 357, Total Reward: -160.8243865966797, Loss: 6.5833516120910645\n",
      "Episode 358, Total Reward: -160.8243865966797, Loss: 3.470141887664795\n",
      "Episode 359, Total Reward: -160.8243865966797, Loss: 8.127060890197754\n",
      "Episode 360, Total Reward: -160.8243865966797, Loss: 0.48219889402389526\n",
      "Episode 361, Total Reward: -160.8243865966797, Loss: 1.9227415323257446\n",
      "Episode 362, Total Reward: -160.8243865966797, Loss: 1.9391287565231323\n",
      "Episode 363, Total Reward: -160.8243865966797, Loss: 1.9594943523406982\n",
      "Episode 364, Total Reward: -160.8243865966797, Loss: 2.2247848510742188\n",
      "Episode 365, Total Reward: -160.8243865966797, Loss: 12.719762802124023\n",
      "Episode 366, Total Reward: -160.8243865966797, Loss: 8.328717231750488\n",
      "Episode 367, Total Reward: -160.8243865966797, Loss: 12.786600112915039\n",
      "Episode 368, Total Reward: -160.8243865966797, Loss: 0.3448808491230011\n",
      "Episode 369, Total Reward: -160.8243865966797, Loss: 8.158832550048828\n",
      "Episode 370, Total Reward: -160.8243865966797, Loss: 0.3619556128978729\n",
      "Episode 371, Total Reward: -160.8243865966797, Loss: 0.3473588824272156\n",
      "Episode 372, Total Reward: -160.8243865966797, Loss: 6.624209403991699\n",
      "Episode 373, Total Reward: -160.8243865966797, Loss: 8.337186813354492\n",
      "Episode 374, Total Reward: -160.8243865966797, Loss: 0.3533024191856384\n",
      "Episode 375, Total Reward: -160.8243865966797, Loss: 2.176128625869751\n",
      "Episode 376, Total Reward: -160.8243865966797, Loss: 0.39199429750442505\n",
      "Episode 377, Total Reward: -160.8243865966797, Loss: 0.3149109482765198\n",
      "Episode 378, Total Reward: -160.8243865966797, Loss: 6.67786979675293\n",
      "Episode 379, Total Reward: -160.8243865966797, Loss: 0.35472995042800903\n",
      "Episode 380, Total Reward: -160.8243865966797, Loss: 0.8718497157096863\n",
      "Episode 381, Total Reward: -160.8243865966797, Loss: 1.9899896383285522\n",
      "Episode 382, Total Reward: -160.8243865966797, Loss: 0.4806853234767914\n",
      "Episode 383, Total Reward: -160.8243865966797, Loss: 0.46667730808258057\n",
      "Episode 384, Total Reward: -160.8243865966797, Loss: 6.592377185821533\n",
      "Episode 385, Total Reward: -160.8243865966797, Loss: 0.39941316843032837\n",
      "Episode 386, Total Reward: -160.8243865966797, Loss: 8.198936462402344\n",
      "Episode 387, Total Reward: -160.8243865966797, Loss: 0.3317789137363434\n",
      "Episode 388, Total Reward: -160.8243865966797, Loss: 0.3492192029953003\n",
      "Episode 389, Total Reward: -160.8243865966797, Loss: 0.35206520557403564\n",
      "Episode 390, Total Reward: -160.8243865966797, Loss: 3.6004526615142822\n",
      "Episode 391, Total Reward: -160.8243865966797, Loss: 8.475798606872559\n",
      "Episode 392, Total Reward: -160.8243865966797, Loss: 0.34211328625679016\n",
      "Episode 393, Total Reward: -160.8243865966797, Loss: 0.32369348406791687\n",
      "Episode 394, Total Reward: -160.8243865966797, Loss: 0.8697091937065125\n",
      "Episode 395, Total Reward: -160.8243865966797, Loss: 0.6095951795578003\n",
      "Episode 396, Total Reward: -160.8243865966797, Loss: 0.3452727794647217\n",
      "Episode 397, Total Reward: -160.8243865966797, Loss: 0.43642908334732056\n",
      "Episode 398, Total Reward: -160.8243865966797, Loss: 0.3879210650920868\n",
      "Episode 399, Total Reward: -160.8243865966797, Loss: 0.3716679513454437\n",
      "Episode 400, Total Reward: -160.8243865966797, Loss: 0.468660831451416\n",
      "Episode 401, Total Reward: -160.8243865966797, Loss: 0.5818015336990356\n",
      "Episode 402, Total Reward: -160.8243865966797, Loss: 8.070924758911133\n",
      "Episode 403, Total Reward: -160.8243865966797, Loss: 1.9349452257156372\n",
      "Episode 404, Total Reward: -160.8243865966797, Loss: 0.37553471326828003\n",
      "Episode 405, Total Reward: -160.8243865966797, Loss: 2.454102039337158\n",
      "Episode 406, Total Reward: -160.8243865966797, Loss: 0.39535778760910034\n",
      "Episode 407, Total Reward: -160.8243865966797, Loss: 0.39934277534484863\n",
      "Episode 408, Total Reward: -160.8243865966797, Loss: 8.15032958984375\n",
      "Episode 409, Total Reward: -160.8243865966797, Loss: 0.6161961555480957\n",
      "Episode 410, Total Reward: -160.8243865966797, Loss: 1.8974965810775757\n",
      "Episode 411, Total Reward: -160.8243865966797, Loss: 6.580672740936279\n",
      "Episode 412, Total Reward: -160.8243865966797, Loss: 9.947480201721191\n",
      "Episode 413, Total Reward: -160.8243865966797, Loss: 0.6616588830947876\n",
      "Episode 414, Total Reward: -160.8243865966797, Loss: 6.626092433929443\n",
      "Episode 415, Total Reward: -160.8243865966797, Loss: 0.5678703188896179\n",
      "Episode 416, Total Reward: -160.8243865966797, Loss: 0.41652700304985046\n",
      "Episode 417, Total Reward: -160.8243865966797, Loss: 6.528702735900879\n",
      "Episode 418, Total Reward: -160.8243865966797, Loss: 6.838141918182373\n",
      "Episode 419, Total Reward: -160.8243865966797, Loss: 6.522130012512207\n",
      "Episode 420, Total Reward: -160.8243865966797, Loss: 0.5827978849411011\n",
      "Episode 421, Total Reward: -160.8243865966797, Loss: 0.33586496114730835\n",
      "Episode 422, Total Reward: -160.8243865966797, Loss: 0.5600354671478271\n",
      "Episode 423, Total Reward: -160.8243865966797, Loss: 6.869183540344238\n",
      "Episode 424, Total Reward: -160.8243865966797, Loss: 2.245490789413452\n",
      "Episode 425, Total Reward: -160.8243865966797, Loss: 1.8984283208847046\n",
      "Episode 426, Total Reward: -160.8243865966797, Loss: 0.45964545011520386\n",
      "Episode 427, Total Reward: -160.8243865966797, Loss: 0.5063744783401489\n",
      "Episode 428, Total Reward: -160.8243865966797, Loss: 0.4042806625366211\n",
      "Episode 429, Total Reward: -160.8243865966797, Loss: 2.0302374362945557\n",
      "Episode 430, Total Reward: -160.8243865966797, Loss: 0.3725786507129669\n",
      "Episode 431, Total Reward: -160.8243865966797, Loss: 1.9627701044082642\n",
      "Episode 432, Total Reward: -160.8243865966797, Loss: 1.980633020401001\n",
      "Episode 433, Total Reward: -160.8243865966797, Loss: 2.0578572750091553\n",
      "Episode 434, Total Reward: -160.8243865966797, Loss: 0.450283020734787\n",
      "Episode 435, Total Reward: -160.8243865966797, Loss: 1.9822206497192383\n",
      "Episode 436, Total Reward: -161.61041259765625, Loss: 0.3783189356327057\n",
      "Episode 437, Total Reward: -160.8243865966797, Loss: 2.1840221881866455\n",
      "Episode 438, Total Reward: -160.8243865966797, Loss: 1.9301714897155762\n",
      "Episode 439, Total Reward: -160.8243865966797, Loss: 8.110477447509766\n",
      "Episode 440, Total Reward: -160.8243865966797, Loss: 2.1812469959259033\n",
      "Episode 441, Total Reward: -160.8243865966797, Loss: 3.547835111618042\n",
      "Episode 442, Total Reward: -160.8243865966797, Loss: 0.9164750576019287\n",
      "Episode 443, Total Reward: -160.8243865966797, Loss: 6.569188117980957\n",
      "Episode 444, Total Reward: -162.0523223876953, Loss: 0.35550862550735474\n",
      "Episode 445, Total Reward: -160.8243865966797, Loss: 1.8773354291915894\n",
      "Episode 446, Total Reward: -160.8243865966797, Loss: 0.5109595060348511\n",
      "Episode 447, Total Reward: -160.8243865966797, Loss: 0.37276706099510193\n",
      "Episode 448, Total Reward: -160.8243865966797, Loss: 0.42081597447395325\n",
      "Episode 449, Total Reward: -160.8243865966797, Loss: 1.954348087310791\n",
      "Episode 450, Total Reward: -162.16981506347656, Loss: 0.41234827041625977\n",
      "Episode 451, Total Reward: -160.8243865966797, Loss: 9.70765209197998\n",
      "Episode 452, Total Reward: -160.8243865966797, Loss: 1.9286667108535767\n",
      "Episode 453, Total Reward: -161.94764709472656, Loss: 8.174066543579102\n",
      "Episode 454, Total Reward: -160.8243865966797, Loss: 8.274271965026855\n",
      "Episode 455, Total Reward: -160.8243865966797, Loss: 6.558266639709473\n",
      "Episode 456, Total Reward: -160.8243865966797, Loss: 0.3834989070892334\n",
      "Episode 457, Total Reward: -160.8243865966797, Loss: 0.4081432819366455\n",
      "Episode 458, Total Reward: -160.8243865966797, Loss: 0.40097129344940186\n",
      "Episode 459, Total Reward: -160.8243865966797, Loss: 8.181660652160645\n",
      "Episode 460, Total Reward: -160.8243865966797, Loss: 1.9214189052581787\n",
      "Episode 461, Total Reward: -160.8243865966797, Loss: 0.36049675941467285\n",
      "Episode 462, Total Reward: -160.8243865966797, Loss: 12.771686553955078\n",
      "Episode 463, Total Reward: -160.8243865966797, Loss: 6.7755231857299805\n",
      "Episode 464, Total Reward: -160.8243865966797, Loss: 0.39564424753189087\n",
      "Episode 465, Total Reward: -160.8243865966797, Loss: 0.3986055850982666\n",
      "Episode 466, Total Reward: -160.8243865966797, Loss: 2.2301840782165527\n",
      "Episode 467, Total Reward: -160.8243865966797, Loss: 0.4538199305534363\n",
      "Episode 468, Total Reward: -160.8243865966797, Loss: 0.36232060194015503\n",
      "Episode 469, Total Reward: -161.50729370117188, Loss: 0.3297126889228821\n",
      "Episode 470, Total Reward: -160.8243865966797, Loss: 0.6209901571273804\n",
      "Episode 471, Total Reward: -161.8976287841797, Loss: 0.36218488216400146\n",
      "Episode 472, Total Reward: -160.8243865966797, Loss: 6.608987331390381\n",
      "Episode 473, Total Reward: -160.8243865966797, Loss: 1.9959145784378052\n",
      "Episode 474, Total Reward: -160.8243865966797, Loss: 6.801948070526123\n",
      "Episode 475, Total Reward: -160.8243865966797, Loss: 0.35681313276290894\n",
      "Episode 476, Total Reward: -160.8243865966797, Loss: 0.378096342086792\n",
      "Episode 477, Total Reward: -160.8243865966797, Loss: 6.921837329864502\n",
      "Episode 478, Total Reward: -160.8243865966797, Loss: 0.3400271534919739\n",
      "Episode 479, Total Reward: -160.8243865966797, Loss: 6.598624229431152\n",
      "Episode 480, Total Reward: -160.8243865966797, Loss: 9.72614574432373\n",
      "Episode 481, Total Reward: -161.72923278808594, Loss: 0.6046118140220642\n",
      "Episode 482, Total Reward: -160.8243865966797, Loss: 0.881231427192688\n",
      "Episode 483, Total Reward: -160.8243865966797, Loss: 0.4549980163574219\n",
      "Episode 484, Total Reward: -160.8243865966797, Loss: 1.934274435043335\n",
      "Episode 485, Total Reward: -160.8243865966797, Loss: 2.269092321395874\n",
      "Episode 486, Total Reward: -160.8243865966797, Loss: 0.35442936420440674\n",
      "Episode 487, Total Reward: -160.8243865966797, Loss: 0.3839385211467743\n",
      "Episode 488, Total Reward: -160.8243865966797, Loss: 0.40757012367248535\n",
      "Episode 489, Total Reward: -160.8243865966797, Loss: 0.6190099716186523\n",
      "Episode 490, Total Reward: -160.8243865966797, Loss: 0.34661629796028137\n",
      "Episode 491, Total Reward: -160.8243865966797, Loss: 0.44130241870880127\n",
      "Episode 492, Total Reward: -160.8243865966797, Loss: 0.4366786777973175\n",
      "Episode 493, Total Reward: -160.8243865966797, Loss: 12.999573707580566\n",
      "Episode 494, Total Reward: -160.8243865966797, Loss: 6.590001583099365\n",
      "Episode 495, Total Reward: -160.8243865966797, Loss: 6.538276672363281\n",
      "Episode 496, Total Reward: -161.8976287841797, Loss: 6.81537389755249\n",
      "Episode 497, Total Reward: -160.8243865966797, Loss: 2.1548337936401367\n",
      "Episode 498, Total Reward: -160.8243865966797, Loss: 0.3391079306602478\n",
      "Episode 499, Total Reward: -160.8243865966797, Loss: 14.549576759338379\n",
      "Episode 500, Total Reward: -160.8243865966797, Loss: 6.653063774108887\n",
      "Model saved to rl_model.pth\n",
      "Model and optimizer loaded successfully.\n",
      "Model loaded from rl_model.pth\n",
      "Assigned bit width: 2, Remaining budget: 1616904192.0\n",
      "Assigned bit width: 2, Remaining budget: 1614807040.0\n",
      "Assigned bit width: 2, Remaining budget: 1612709888.0\n",
      "Assigned bit width: 2, Remaining budget: 1610612736.0\n",
      "Assigned bit width: 2, Remaining budget: 1604976640.0\n",
      "Assigned bit width: 2, Remaining budget: 1599340544.0\n",
      "Assigned bit width: 2, Remaining budget: 1593704448.0\n",
      "Assigned bit width: 2, Remaining budget: 1591607296.0\n",
      "Assigned bit width: 2, Remaining budget: 1589510144.0\n",
      "Assigned bit width: 2, Remaining budget: 1587412992.0\n",
      "Assigned bit width: 2, Remaining budget: 1585315840.0\n",
      "Assigned bit width: 2, Remaining budget: 1579679744.0\n",
      "Assigned bit width: 2, Remaining budget: 1574043648.0\n",
      "Assigned bit width: 2, Remaining budget: 1568407552.0\n",
      "Assigned bit width: 2, Remaining budget: 1566310400.0\n",
      "Assigned bit width: 2, Remaining budget: 1564213248.0\n",
      "Assigned bit width: 2, Remaining budget: 1562116096.0\n",
      "Assigned bit width: 2, Remaining budget: 1560018944.0\n",
      "Assigned bit width: 2, Remaining budget: 1554382848.0\n",
      "Assigned bit width: 2, Remaining budget: 1548746752.0\n",
      "Assigned bit width: 2, Remaining budget: 1543110656.0\n",
      "Assigned bit width: 2, Remaining budget: 1541013504.0\n",
      "Assigned bit width: 2, Remaining budget: 1538916352.0\n",
      "Assigned bit width: 2, Remaining budget: 1536819200.0\n",
      "Assigned bit width: 2, Remaining budget: 1534722048.0\n",
      "Assigned bit width: 2, Remaining budget: 1529085952.0\n",
      "Assigned bit width: 2, Remaining budget: 1523449856.0\n",
      "Assigned bit width: 2, Remaining budget: 1517813760.0\n",
      "Assigned bit width: 2, Remaining budget: 1515716608.0\n",
      "Assigned bit width: 2, Remaining budget: 1513619456.0\n",
      "Assigned bit width: 2, Remaining budget: 1511522304.0\n",
      "Assigned bit width: 2, Remaining budget: 1509425152.0\n",
      "Assigned bit width: 2, Remaining budget: 1503789056.0\n",
      "Assigned bit width: 2, Remaining budget: 1498152960.0\n",
      "Assigned bit width: 2, Remaining budget: 1492516864.0\n",
      "Assigned bit width: 2, Remaining budget: 1490419712.0\n",
      "Assigned bit width: 2, Remaining budget: 1488322560.0\n",
      "Assigned bit width: 2, Remaining budget: 1486225408.0\n",
      "Assigned bit width: 2, Remaining budget: 1484128256.0\n",
      "Assigned bit width: 2, Remaining budget: 1478492160.0\n",
      "Assigned bit width: 2, Remaining budget: 1472856064.0\n",
      "Assigned bit width: 2, Remaining budget: 1467219968.0\n",
      "Assigned bit width: 2, Remaining budget: 1465122816.0\n",
      "Assigned bit width: 2, Remaining budget: 1463025664.0\n",
      "Assigned bit width: 2, Remaining budget: 1460928512.0\n",
      "Assigned bit width: 2, Remaining budget: 1458831360.0\n",
      "Assigned bit width: 2, Remaining budget: 1453195264.0\n",
      "Assigned bit width: 2, Remaining budget: 1447559168.0\n",
      "Assigned bit width: 2, Remaining budget: 1441923072.0\n",
      "Assigned bit width: 2, Remaining budget: 1439825920.0\n",
      "Assigned bit width: 2, Remaining budget: 1437728768.0\n",
      "Assigned bit width: 2, Remaining budget: 1435631616.0\n",
      "Assigned bit width: 2, Remaining budget: 1433534464.0\n",
      "Assigned bit width: 2, Remaining budget: 1427898368.0\n",
      "Assigned bit width: 2, Remaining budget: 1422262272.0\n",
      "Assigned bit width: 2, Remaining budget: 1416626176.0\n",
      "Assigned bit width: 2, Remaining budget: 1414529024.0\n",
      "Assigned bit width: 2, Remaining budget: 1412431872.0\n",
      "Assigned bit width: 2, Remaining budget: 1410334720.0\n",
      "Assigned bit width: 2, Remaining budget: 1408237568.0\n",
      "Assigned bit width: 2, Remaining budget: 1402601472.0\n",
      "Assigned bit width: 2, Remaining budget: 1396965376.0\n",
      "Assigned bit width: 2, Remaining budget: 1391329280.0\n",
      "Assigned bit width: 2, Remaining budget: 1389232128.0\n",
      "Assigned bit width: 2, Remaining budget: 1387134976.0\n",
      "Assigned bit width: 2, Remaining budget: 1385037824.0\n",
      "Assigned bit width: 2, Remaining budget: 1382940672.0\n",
      "Assigned bit width: 2, Remaining budget: 1377304576.0\n",
      "Assigned bit width: 2, Remaining budget: 1371668480.0\n",
      "Assigned bit width: 2, Remaining budget: 1366032384.0\n",
      "Assigned bit width: 2, Remaining budget: 1363935232.0\n",
      "Assigned bit width: 2, Remaining budget: 1361838080.0\n",
      "Assigned bit width: 2, Remaining budget: 1359740928.0\n",
      "Assigned bit width: 2, Remaining budget: 1357643776.0\n",
      "Assigned bit width: 2, Remaining budget: 1352007680.0\n",
      "Assigned bit width: 2, Remaining budget: 1346371584.0\n",
      "Assigned bit width: 2, Remaining budget: 1340735488.0\n",
      "Assigned bit width: 2, Remaining budget: 1338638336.0\n",
      "Assigned bit width: 2, Remaining budget: 1336541184.0\n",
      "Assigned bit width: 2, Remaining budget: 1334444032.0\n",
      "Assigned bit width: 2, Remaining budget: 1332346880.0\n",
      "Assigned bit width: 2, Remaining budget: 1326710784.0\n",
      "Assigned bit width: 2, Remaining budget: 1321074688.0\n",
      "Assigned bit width: 2, Remaining budget: 1315438592.0\n",
      "Assigned bit width: 2, Remaining budget: 1313341440.0\n",
      "Assigned bit width: 2, Remaining budget: 1311244288.0\n",
      "Assigned bit width: 2, Remaining budget: 1309147136.0\n",
      "Assigned bit width: 2, Remaining budget: 1307049984.0\n",
      "Assigned bit width: 2, Remaining budget: 1301413888.0\n",
      "Assigned bit width: 2, Remaining budget: 1295777792.0\n",
      "Assigned bit width: 2, Remaining budget: 1290141696.0\n",
      "Assigned bit width: 2, Remaining budget: 1288044544.0\n",
      "Assigned bit width: 2, Remaining budget: 1285947392.0\n",
      "Assigned bit width: 2, Remaining budget: 1283850240.0\n",
      "Assigned bit width: 2, Remaining budget: 1281753088.0\n",
      "Assigned bit width: 2, Remaining budget: 1276116992.0\n",
      "Assigned bit width: 2, Remaining budget: 1270480896.0\n",
      "Assigned bit width: 2, Remaining budget: 1264844800.0\n",
      "Assigned bit width: 2, Remaining budget: 1262747648.0\n",
      "Assigned bit width: 2, Remaining budget: 1260650496.0\n",
      "Assigned bit width: 2, Remaining budget: 1258553344.0\n",
      "Assigned bit width: 2, Remaining budget: 1256456192.0\n",
      "Assigned bit width: 2, Remaining budget: 1250820096.0\n",
      "Assigned bit width: 2, Remaining budget: 1245184000.0\n",
      "Assigned bit width: 2, Remaining budget: 1239547904.0\n",
      "Assigned bit width: 2, Remaining budget: 1237450752.0\n",
      "Assigned bit width: 2, Remaining budget: 1235353600.0\n",
      "Assigned bit width: 2, Remaining budget: 1233256448.0\n",
      "Assigned bit width: 2, Remaining budget: 1231159296.0\n",
      "Assigned bit width: 2, Remaining budget: 1225523200.0\n",
      "Assigned bit width: 2, Remaining budget: 1219887104.0\n",
      "Assigned bit width: 2, Remaining budget: 1214251008.0\n",
      "Assigned bit width: 2, Remaining budget: 1212153856.0\n",
      "Assigned bit width: 2, Remaining budget: 1210056704.0\n",
      "Assigned bit width: 2, Remaining budget: 1207959552.0\n",
      "Assigned bit width: 2, Remaining budget: 1205862400.0\n",
      "Assigned bit width: 2, Remaining budget: 1200226304.0\n",
      "Assigned bit width: 2, Remaining budget: 1194590208.0\n",
      "Assigned bit width: 2, Remaining budget: 1188954112.0\n",
      "Assigned bit width: 2, Remaining budget: 1186856960.0\n",
      "Assigned bit width: 2, Remaining budget: 1184759808.0\n",
      "Assigned bit width: 2, Remaining budget: 1182662656.0\n",
      "Assigned bit width: 2, Remaining budget: 1180565504.0\n",
      "Assigned bit width: 2, Remaining budget: 1174929408.0\n",
      "Assigned bit width: 2, Remaining budget: 1169293312.0\n",
      "Assigned bit width: 2, Remaining budget: 1163657216.0\n",
      "Assigned bit width: 2, Remaining budget: 1161560064.0\n",
      "Assigned bit width: 2, Remaining budget: 1159462912.0\n",
      "Assigned bit width: 2, Remaining budget: 1157365760.0\n",
      "Assigned bit width: 2, Remaining budget: 1155268608.0\n",
      "Assigned bit width: 2, Remaining budget: 1149632512.0\n",
      "Assigned bit width: 2, Remaining budget: 1143996416.0\n",
      "Assigned bit width: 2, Remaining budget: 1138360320.0\n",
      "Assigned bit width: 2, Remaining budget: 1136263168.0\n",
      "Assigned bit width: 2, Remaining budget: 1134166016.0\n",
      "Assigned bit width: 2, Remaining budget: 1132068864.0\n",
      "Assigned bit width: 2, Remaining budget: 1129971712.0\n",
      "Assigned bit width: 2, Remaining budget: 1124335616.0\n",
      "Assigned bit width: 2, Remaining budget: 1118699520.0\n",
      "Assigned bit width: 2, Remaining budget: 1113063424.0\n",
      "Assigned bit width: 2, Remaining budget: 1110966272.0\n",
      "Assigned bit width: 2, Remaining budget: 1108869120.0\n",
      "Assigned bit width: 2, Remaining budget: 1106771968.0\n",
      "Assigned bit width: 2, Remaining budget: 1104674816.0\n",
      "Assigned bit width: 2, Remaining budget: 1099038720.0\n",
      "Assigned bit width: 2, Remaining budget: 1093402624.0\n",
      "Assigned bit width: 2, Remaining budget: 1087766528.0\n",
      "Assigned bit width: 2, Remaining budget: 1085669376.0\n",
      "Assigned bit width: 2, Remaining budget: 1083572224.0\n",
      "Assigned bit width: 2, Remaining budget: 1081475072.0\n",
      "Assigned bit width: 2, Remaining budget: 1079377920.0\n",
      "Assigned bit width: 2, Remaining budget: 1073741824.0\n",
      "Assigned bit width: 2, Remaining budget: 1068105728.0\n",
      "Assigned bit width: 2, Remaining budget: 1062469632.0\n",
      "Assigned bit width: 2, Remaining budget: 1060372480.0\n",
      "Assigned bit width: 2, Remaining budget: 1058275328.0\n",
      "Assigned bit width: 2, Remaining budget: 1056178176.0\n",
      "Assigned bit width: 2, Remaining budget: 1054081024.0\n",
      "Assigned bit width: 2, Remaining budget: 1048444928.0\n",
      "Assigned bit width: 2, Remaining budget: 1042808832.0\n",
      "Assigned bit width: 2, Remaining budget: 1037172736.0\n",
      "Assigned bit width: 2, Remaining budget: 1035075584.0\n",
      "Assigned bit width: 2, Remaining budget: 1032978432.0\n",
      "Assigned bit width: 2, Remaining budget: 1030881280.0\n",
      "Assigned bit width: 2, Remaining budget: 1028784128.0\n",
      "Assigned bit width: 2, Remaining budget: 1023148032.0\n",
      "Assigned bit width: 2, Remaining budget: 1017511936.0\n",
      "Assigned bit width: 2, Remaining budget: 1011875840.0\n",
      "Assigned bit width: 2, Remaining budget: 1009778688.0\n",
      "Assigned bit width: 2, Remaining budget: 1007681536.0\n",
      "Assigned bit width: 2, Remaining budget: 1005584384.0\n",
      "Assigned bit width: 2, Remaining budget: 1003487232.0\n",
      "Assigned bit width: 2, Remaining budget: 997851136.0\n",
      "Assigned bit width: 2, Remaining budget: 992215040.0\n",
      "Assigned bit width: 2, Remaining budget: 986578944.0\n",
      "Assigned bit width: 2, Remaining budget: 984481792.0\n",
      "Assigned bit width: 2, Remaining budget: 982384640.0\n",
      "Assigned bit width: 2, Remaining budget: 980287488.0\n",
      "Assigned bit width: 2, Remaining budget: 978190336.0\n",
      "Assigned bit width: 2, Remaining budget: 972554240.0\n",
      "Assigned bit width: 2, Remaining budget: 966918144.0\n",
      "Assigned bit width: 2, Remaining budget: 961282048.0\n",
      "Assigned bit width: 2, Remaining budget: 959184896.0\n",
      "Assigned bit width: 2, Remaining budget: 957087744.0\n",
      "Assigned bit width: 2, Remaining budget: 954990592.0\n",
      "Assigned bit width: 2, Remaining budget: 952893440.0\n",
      "Assigned bit width: 2, Remaining budget: 947257344.0\n",
      "Assigned bit width: 2, Remaining budget: 941621248.0\n",
      "Assigned bit width: 2, Remaining budget: 935985152.0\n",
      "Assigned bit width: 2, Remaining budget: 933888000.0\n",
      "Assigned bit width: 2, Remaining budget: 931790848.0\n",
      "Assigned bit width: 2, Remaining budget: 929693696.0\n",
      "Assigned bit width: 2, Remaining budget: 927596544.0\n",
      "Assigned bit width: 2, Remaining budget: 921960448.0\n",
      "Assigned bit width: 2, Remaining budget: 916324352.0\n",
      "Assigned bit width: 2, Remaining budget: 910688256.0\n",
      "Assigned bit width: 2, Remaining budget: 908591104.0\n",
      "Assigned bit width: 2, Remaining budget: 906493952.0\n",
      "Assigned bit width: 2, Remaining budget: 904396800.0\n",
      "Assigned bit width: 2, Remaining budget: 902299648.0\n",
      "Assigned bit width: 2, Remaining budget: 896663552.0\n",
      "Assigned bit width: 2, Remaining budget: 891027456.0\n",
      "Assigned bit width: 2, Remaining budget: 885391360.0\n",
      "Assigned bit width: 2, Remaining budget: 883294208.0\n",
      "Assigned bit width: 2, Remaining budget: 881197056.0\n",
      "Assigned bit width: 2, Remaining budget: 879099904.0\n",
      "Assigned bit width: 2, Remaining budget: 877002752.0\n",
      "Assigned bit width: 2, Remaining budget: 871366656.0\n",
      "Assigned bit width: 2, Remaining budget: 865730560.0\n",
      "Assigned bit width: 2, Remaining budget: 860094464.0\n",
      "Assigned bit width: 2, Remaining budget: 857997312.0\n",
      "Assigned bit width: 2, Remaining budget: 855900160.0\n",
      "Assigned bit width: 2, Remaining budget: 853803008.0\n",
      "Assigned bit width: 2, Remaining budget: 851705856.0\n",
      "Assigned bit width: 2, Remaining budget: 846069760.0\n",
      "Assigned bit width: 2, Remaining budget: 840433664.0\n",
      "Assigned bit width: 2, Remaining budget: 834797568.0\n",
      "Assigned bit width: 2, Remaining budget: 832700416.0\n",
      "Assigned bit width: 2, Remaining budget: 830603264.0\n",
      "Assigned bit width: 2, Remaining budget: 828506112.0\n",
      "Assigned bit width: 2, Remaining budget: 826408960.0\n",
      "Assigned bit width: 2, Remaining budget: 820772864.0\n",
      "Assigned bit width: 2, Remaining budget: 815136768.0\n",
      "Assigned bit width: 2, Remaining budget: 809500672.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    # 加载json文件数据\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    data = []\n",
    "    # 遍历字典中的值并平铺\n",
    "    for id, block in enumerate(json_data):\n",
    "        for key, value in json_data[block].items():\n",
    "            data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "bits = [2, 3, 4, 8]  # 可选位宽\n",
    "F = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/fisher_data.json')\n",
    "F = torch.tensor(F, dtype=torch.float32)\n",
    "P = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/LayersParams.json')\n",
    "p = torch.tensor(P, dtype=torch.float32)\n",
    "N = len(F)  # 层数\n",
    "B = 16  # 原始位宽\n",
    "R = 0.25  # 压缩率\n",
    "alpha = 0.4  # 目标函数中的衰减系数\n",
    "\n",
    "# 初始化 RL 类\n",
    "rl = RL(bits, F, P, N, B, R, alpha)\n",
    "\n",
    "# 训练\n",
    "reward_history = rl.train(num_episodes=500, save_path=\"rl_model.pth\")\n",
    "\n",
    "# 测试\n",
    "rl.test(load_path=\"rl_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04c4d8-78ca-49db-8c38-738131358f9f",
   "metadata": {},
   "source": [
    "# LSTM 状态更新为之前的模型分配结果 + 剩余压缩预算 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9af4d212-2ce8-48ad-8e42-06806d89fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "class QuantizationEnv:\n",
    "    def __init__(self, layer_sizes, bits, F, alpha=1.0, original_bit=16, R=0.5, beta=1.0):\n",
    "        \"\"\"\n",
    "        初始化量化环境\n",
    "        \n",
    "        参数：\n",
    "        layer_sizes: 各层大小列表\n",
    "        bits: 可选位宽列表\n",
    "        F: 各层的量化敏感性参数列表\n",
    "        alpha: 衰减系数\n",
    "        original_bit: 原始位宽\n",
    "        R: 压缩率\n",
    "        beta: 精度损失的权重系数\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.bits = bits\n",
    "        self.F = F\n",
    "        self.alpha = alpha\n",
    "        self.original_bit = original_bit\n",
    "        self.target_bit = original_bit * R\n",
    "        self.beta = beta\n",
    "        self.total_size = sum(layer_sizes)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"重置环境\"\"\"\n",
    "        self.current_layer = 0\n",
    "        self.allocations = []\n",
    "        self.remaining_budget = self.total_size * self.target_bit\n",
    "        self.done = False\n",
    "        print(self.current_layer)\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"状态特征设计\"\"\"\n",
    "        return np.array([\n",
    "            self.current_layer / len(self.layer_sizes),\n",
    "            self.remaining_budget / (self.total_size * self.original_bit),\n",
    "            self.layer_sizes[self.current_layer] / max(self.layer_sizes),\n",
    "            self.F[self.current_layer] / max(self.F)  # 量化敏感性归一化\n",
    "        ])\n",
    "    \n",
    "    def _calc_accuracy_loss(self):\n",
    "        \"\"\"计算当前分配的精度损失\"\"\"\n",
    "        total_loss = 0\n",
    "        for i, bit in enumerate(self.allocations):\n",
    "            total_loss += self.F[i] * np.exp(-self.alpha * (self.original_bit / bit))\n",
    "        return total_loss\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"执行一步动作\"\"\"\n",
    "        assert not self.done, \"Episode already finished\"\n",
    "        \n",
    "        # 计算当前层位宽分配消耗的预算\n",
    "        layer_size = self.layer_sizes[self.current_layer]\n",
    "        bit_cost = layer_size * self.bits[action]\n",
    "        self.remaining_budget -= bit_cost\n",
    "        self.allocations.append(self.bits[action])\n",
    "        \n",
    "        # 更新状态\n",
    "        self.current_layer += 1\n",
    "        if self.current_layer >= len(self.layer_sizes):\n",
    "            self.done = True\n",
    "            # 计算最终平均位宽\n",
    "            avg_bit = sum(a * s for a, s in zip(self.allocations, self.layer_sizes)) / self.total_size\n",
    "            # 计算精度损失\n",
    "            accuracy_loss = self._calc_accuracy_loss()\n",
    "            # 最终奖励\n",
    "            if avg_bit <= self.target_bit:\n",
    "                reward = 100 - self.beta * accuracy_loss  # 满足压缩率约束\n",
    "            else:\n",
    "                reward = -10 * abs(avg_bit - self.target_bit) - self.beta * accuracy_loss  # 超出部分惩罚\n",
    "        else:\n",
    "            reward = 0  # 中间步骤无即时奖励\n",
    "        \n",
    "        return self._get_state(), reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb92e70e-689b-4227-87f8-a06b652b6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, lstm_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=state_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        self.hidden = None\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        action_probs = torch.softmax(self.actor(lstm_out), dim=-1)\n",
    "        state_value = self.critic(lstm_out)\n",
    "        return action_probs, state_value, hidden\n",
    "        \n",
    "class LSTM_PPO:           \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lstm_layers=2):\n",
    "        self.policy = ActorCritic(state_dim, action_dim, hidden_dim, lstm_layers)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "        \n",
    "        # 超参数\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "        self.K_epochs = 4\n",
    "        self.memory = []\n",
    "    \n",
    "    def select_action(self, state, hidden):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)  # (1,1,state_dim)\n",
    "        with torch.no_grad():\n",
    "            probs, value, new_hidden = self.policy(state, hidden)\n",
    "        dist = Categorical(probs.squeeze())\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), value, new_hidden\n",
    "    \n",
    "    def update(self):\n",
    "        # 转换数据为张量\n",
    "        states = torch.stack([m['state'] for m in self.memory])\n",
    "        actions = torch.tensor([m['action'] for m in self.memory])\n",
    "        old_log_probs = torch.stack([m['log_prob'] for m in self.memory])\n",
    "        rewards = self._calc_discounted_rewards()\n",
    "        \n",
    "        # PPO优化循环\n",
    "        for _ in range(self.K_epochs):\n",
    "            probs, values, _ = self.policy(states.unsqueeze(1))\n",
    "            dist = Categorical(probs.squeeze())\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # 计算损失\n",
    "            ratios = torch.exp(dist.log_prob(actions) - old_log_probs.detach())\n",
    "            advantages = rewards - values.squeeze().detach()\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = torch.nn.functional.mse_loss(values.squeeze(), rewards)\n",
    "            loss = actor_loss + 0.5*critic_loss - 0.01*entropy\n",
    "            \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.memory = []\n",
    "    \n",
    "    def _calc_discounted_rewards(self):\n",
    "        rewards = [m['reward'] for m in self.memory]\n",
    "        discounted = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.gamma * R\n",
    "            discounted.insert(0, R)\n",
    "        return torch.tensor(discounted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b76d91ae-f2f2-4613-8bfa-203a0625fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(layer_sizes, bits, F, alpha, R, episodes=1000):\n",
    "    # 初始化环境与智能体\n",
    "    env = QuantizationEnv(layer_sizes, bits, F, alpha, R=R)\n",
    "    agent = LSTM_PPO(state_dim=4, action_dim=len(bits))\n",
    "    \n",
    "    # 训练循环\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        hidden = None\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # 选择动作\n",
    "            action, log_prob, value, hidden = agent.select_action(state, hidden)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # 存储经验\n",
    "            agent.memory.append({\n",
    "                'state': torch.FloatTensor(state),\n",
    "                'action': action,\n",
    "                'log_prob': log_prob,\n",
    "                'reward': reward\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # 策略更新\n",
    "        agent.update()\n",
    "        \n",
    "        # 打印进度\n",
    "        if (ep+1) % 50 == 0:\n",
    "            avg_bit = sum(env.allocations)/len(env.allocations)\n",
    "            accuracy_loss = env._calc_accuracy_loss()\n",
    "            print(f\"Ep {ep+1}: Reward={episode_reward:.1f}, Avg Bit={avg_bit:.1f}, Accuracy Loss={accuracy_loss:.2f}\")\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eae46-1969-4dec-befc-93d0d175365d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    # 加载json文件数据\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    data = []\n",
    "    # 遍历字典中的值并平铺\n",
    "    for id, block in enumerate(json_data):\n",
    "        for key, value in json_data[block].items():\n",
    "            data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "bits = [2, 3, 4, 8]  # 可选位宽\n",
    "F = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/fisher_data.json')\n",
    "P = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/LayersParams.json')\n",
    "N = len(F)  # 层数\n",
    "B = 16  # 原始位宽\n",
    "R = 0.25  # 压缩率\n",
    "alpha = 0.4  # 目标函数中的衰减系数\n",
    "\n",
    "# 初始化 RL 类\n",
    "train(P, bits, F, alpha, R, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ff5f1-f5c8-4f5a-99c8-e1558183cfd6",
   "metadata": {},
   "source": [
    "# 新版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b40d53-ce0a-4e01-bfe5-7b706df2910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000 | Total Reward: -1.41\n",
      "Episode 200/1000 | Total Reward: 9.05\n",
      "Episode 300/1000 | Total Reward: -12.92\n",
      "Episode 400/1000 | Total Reward: 4.49\n",
      "Episode 500/1000 | Total Reward: -0.62\n",
      "Episode 600/1000 | Total Reward: 1.39\n",
      "Episode 700/1000 | Total Reward: -5.61\n",
      "Episode 800/1000 | Total Reward: 3.32\n",
      "Episode 900/1000 | Total Reward: 7.00\n",
      "Episode 1000/1000 | Total Reward: -7.76\n",
      "\n",
      "Final Bit Allocation:\n",
      "Layers: 224\n",
      "Allocated Bits: [8, 2, 8, 4, 8, 8, 4, 3, 3, 4, 8, 4, 3, 2, 4, 4, 2, 2, 4, 4, 8, 2, 2, 3, 2, 4, 4, 3, 4, 4, 8, 2, 3, 2, 2, 2, 4, 2, 4, 3, 2, 8, 3, 2, 4, 2, 2, 4, 8, 4, 4, 3, 2, 3, 8, 2, 4, 8, 4, 2, 2, 2, 2, 3, 4, 8, 3, 8, 8, 8, 2, 3, 4, 3, 4, 4, 2, 3, 2, 8, 8, 3, 3, 8, 4, 4, 3, 3, 8, 4, 4, 8, 4, 8, 3, 2, 4, 8, 2, 8, 4, 8, 4, 8, 2, 2, 2, 3, 8, 2, 3, 4, 8, 8, 4, 3, 4, 4, 3, 4, 8, 3, 3, 4, 2, 8, 4, 2, 3, 3, 4, 8, 4, 3, 8, 2, 4, 3, 3, 2, 8, 4, 3, 8, 4, 2, 4, 4, 4, 2, 3, 4, 4, 4, 8, 4, 8, 2, 3, 3, 3, 4, 8, 8, 2, 2, 4, 4, 8, 4, 3, 4, 4, 2, 4, 3, 3, 3, 2, 3, 8, 2, 8, 4, 8, 8, 8, 4, 2, 2, 3, 8, 2, 8, 8, 4, 8, 8, 3, 2, 8, 2, 2, 8, 8, 8, 2, 3, 8, 8, 2, 2, 3, 2, 2, 4, 4, 8, 3, 3, 8, 3, 8, 4]\n",
      "Total Usage: 28000124928 bits\n",
      "Original Usage: 207232172032.0 bits\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bit_allocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 253\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([l\u001b[38;5;241m*\u001b[39mb\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ml,b\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mzip\u001b[39m(layer_sizes,allocations)])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_env\u001b[38;5;241m.\u001b[39moriginal_size\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(layer_sizes\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m(\u001b[43mbit_allocation\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m16\u001b[39m))\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(layer_sizes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bit_allocation' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# 环境参数配置\n",
    "import random\n",
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    # 加载json文件数据\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    data = []\n",
    "    # 遍历字典中的值并平铺\n",
    "    for id, block in enumerate(json_data):\n",
    "        for key, value in json_data[block].items():\n",
    "            data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "bits = [2, 3, 4, 8]  # 可选位宽\n",
    "F = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/fisher_data.json')\n",
    "layer_sizes = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/LayersParams.json')\n",
    "N = len(F)  # 层数\n",
    "R = 0.25  # 压缩率\n",
    "alpha = 2  # 目标函数中的衰减系数\n",
    "\n",
    "\n",
    "# 超参数\n",
    "EPISODES = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "CLIP_EPSILON = 0.2\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 3e-4\n",
    "UPDATE_ITERS = 10\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BitAllocationEnv:\n",
    "    def __init__(self, layer_sizes, bits, F, alpha, R):\n",
    "        # 将numpy数组转换为GPU张量\n",
    "        self.layer_sizes = torch.tensor(layer_sizes, dtype=torch.float32, device=device)\n",
    "        self.bits = torch.tensor(bits, dtype=torch.float32, device=device)\n",
    "        self.F = torch.tensor(F, dtype=torch.float32, device=device)\n",
    "        self.alpha = alpha\n",
    "        self.R = R\n",
    "        self.original_size = torch.sum(self.layer_sizes) * 32  # GPU计算\n",
    "        self.max_budget = self.original_size * R\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_layer = 0\n",
    "        self.allocated_bits = []\n",
    "        self.used_budget = torch.tensor(0.0, device=device)  # GPU张量\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"状态编码：全部使用GPU张量操作\"\"\"\n",
    "        state = torch.zeros(3 + self.n_layers * 2, device=device)\n",
    "        \n",
    "        # 当前进度\n",
    "        state[0] = self.current_layer / self.n_layers\n",
    "        \n",
    "        # 预算使用率\n",
    "        state[1] = self.used_budget / self.max_budget\n",
    "        \n",
    "        # Fisher信息\n",
    "        state[2:2+self.n_layers] = self.F / torch.max(self.F)\n",
    "        \n",
    "        # 层大小归一化\n",
    "        state[3+self.n_layers:] = self.layer_sizes / torch.max(self.layer_sizes)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"全部使用GPU张量计算\"\"\"\n",
    "        bit_value = self.bits[action]\n",
    "        layer_size = self.layer_sizes[self.current_layer]\n",
    "        \n",
    "        new_usage = self.used_budget + layer_size * bit_value\n",
    "        \n",
    "        if new_usage > self.max_budget:\n",
    "            reward = torch.tensor(-1000.0, device=device)\n",
    "            done = True\n",
    "        else:\n",
    "            self.allocated_bits.append(bit_value.item())  # 记录时转CPU\n",
    "            self.used_budget = new_usage\n",
    "            self.current_layer += 1\n",
    "            reward = torch.tensor(0.1 if self.current_layer < self.n_layers else 0.0, device=device)\n",
    "            done = self.current_layer >= self.n_layers\n",
    "            \n",
    "            if done:\n",
    "                reward = self._calculate_final_reward()\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _calculate_final_reward(self):\n",
    "        \"\"\"GPU加速的奖励计算\"\"\"\n",
    "        allocated_bits_tensor = torch.tensor(self.allocated_bits, device=device)\n",
    "        loss = torch.sum(self.F * torch.exp(-self.alpha * allocated_bits_tensor))\n",
    "        return -loss\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.to(device)  # 确保网络在GPU上\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1))\n",
    "        self.to(device)  # 确保网络在GPU上\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.buffer = deque(maxlen=10000)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # 批量数据直接从GPU获取\n",
    "        batch = random.sample(self.buffer, BATCH_SIZE)\n",
    "        states = torch.stack([t[0] for t in batch]).to(device)\n",
    "        actions = torch.tensor([t[1] for t in batch], dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor([t[2] for t in batch], dtype=torch.float32, device=device)\n",
    "        next_states = torch.stack([t[3] for t in batch]).to(device)\n",
    "        dones = torch.tensor([t[4] for t in batch], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # 价值计算\n",
    "        with torch.no_grad():\n",
    "            target_v = rewards + GAMMA * (1 - dones) * self.critic(next_states).squeeze()\n",
    "        \n",
    "        # 优势计算\n",
    "        V = self.critic(states).squeeze()\n",
    "        advantage = (target_v - V).detach()\n",
    "        \n",
    "        # Critic更新\n",
    "        critic_loss = nn.MSELoss()(V, target_v)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # Actor更新\n",
    "        old_probs = self.actor(states).detach()\n",
    "        old_probs = old_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        for _ in range(UPDATE_ITERS):\n",
    "            new_probs = self.actor(states)\n",
    "            new_dist = torch.distributions.Categorical(new_probs)\n",
    "            log_probs = new_dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(log_probs - torch.log(old_probs))\n",
    "            clipped_ratio = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON)\n",
    "            \n",
    "            actor_loss = -torch.min(ratio*advantage, clipped_ratio*advantage).mean()\n",
    "            \n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "# 初始化环境和Agent\n",
    "env = BitAllocationEnv(layer_sizes, bits, F, alpha, R)\n",
    "state_dim = env._get_state().shape[0]\n",
    "action_dim = len(bits)\n",
    "agent = PPO(state_dim, action_dim)\n",
    "\n",
    "# 训练循环（完全GPU加速）\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # 存储GPU张量（注意：done需要转CPU）\n",
    "        agent.buffer.append((\n",
    "            state.cpu().detach(),  # 使用CPU存储减少显存占用\n",
    "            action,\n",
    "            reward.cpu().item(),\n",
    "            next_state.cpu().detach(),\n",
    "            done\n",
    "        ))\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward.item()\n",
    "    \n",
    "    # 更新策略\n",
    "    agent.update()\n",
    "    \n",
    "    # 打印进度\n",
    "    if (episode+1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}/{EPISODES} | Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# 测试阶段（保持GPU计算）\n",
    "with torch.no_grad():\n",
    "    test_env = BitAllocationEnv(layer_sizes, bits, F, alpha, R)\n",
    "    state = test_env.reset()\n",
    "    allocations = []\n",
    "    \n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = test_env.step(action)\n",
    "        allocations.append(bits[action])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "# 结果展示（转CPU处理）\n",
    "print(\"\\nFinal Bit Allocation:\")\n",
    "print(f\"Layers: {len(layer_sizes)}\")\n",
    "print(f\"Allocated Bits: {allocations}\")\n",
    "print(f\"Total Usage: {sum([l*b for l,b in zip(layer_sizes,allocations)])} bits\")\n",
    "print(f\"Original Usage: {test_env.original_size.item()} bits\")\n",
    "print(f\"Compression Rate: {np.sum(layer_sizes * (np.array(allocations) / 16))/np.sum(layer_sizes):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d9b088-93c8-4378-b262-0cea3d4d21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Rate: 27.02%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Compression Rate: {np.sum(layer_sizes * (np.array(allocations) / 16))/np.sum(layer_sizes):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8cbfc-555e-4f86-a9ab-e321741cdbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# 环境参数配置\n",
    "import random\n",
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    # 加载json文件数据\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    data = []\n",
    "    # 遍历字典中的值并平铺\n",
    "    for id, block in enumerate(json_data):\n",
    "        for key, value in json_data[block].items():\n",
    "            data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "bits = [2, 3, 4, 8]  # 可选位宽\n",
    "F = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/fisher_data.json')\n",
    "layer_sizes = load_json('/root/autodl-tmp/methods/mix_quantize/model_info/llama2-7b/LayersParams.json')\n",
    "N = len(F)  # 层数\n",
    "R = 0.25  # 压缩率\n",
    "alpha = 2  # 目标函数中的衰减系数\n",
    "\n",
    "\n",
    "# 超参数\n",
    "EPISODES = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "CLIP_EPSILON = 0.2\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 3e-4\n",
    "UPDATE_ITERS = 10\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BitAllocationEnv:\n",
    "    def __init__(self, layer_sizes, bits, F, alpha, R):\n",
    "        # 将numpy数组转换为GPU张量\n",
    "        self.layer_sizes = torch.tensor(layer_sizes, dtype=torch.float32, device=device)\n",
    "        self.bits = torch.tensor(bits, dtype=torch.float32, device=device)\n",
    "        self.F = torch.tensor(F, dtype=torch.float32, device=device)\n",
    "        self.alpha = alpha\n",
    "        self.R = R\n",
    "        self.original_size = torch.sum(self.layer_sizes) * 32  # GPU计算\n",
    "        self.max_budget = self.original_size * R\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_layer = 0\n",
    "        self.allocated_bits = []\n",
    "        self.used_budget = torch.tensor(0.0, device=device)  # GPU张量\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"状态编码：全部使用GPU张量操作\"\"\"\n",
    "        state = torch.zeros(3 + self.n_layers * 2, device=device)\n",
    "        \n",
    "        # 当前进度\n",
    "        state[0] = self.current_layer / self.n_layers\n",
    "        \n",
    "        # 预算使用率\n",
    "        state[1] = self.used_budget / self.max_budget\n",
    "        \n",
    "        # Fisher信息\n",
    "        state[2:2+self.n_layers] = self.F / torch.max(self.F)\n",
    "        \n",
    "        # 层大小归一化\n",
    "        state[3+self.n_layers:] = self.layer_sizes / torch.max(self.layer_sizes)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"全部使用GPU张量计算\"\"\"\n",
    "        bit_value = self.bits[action]\n",
    "        layer_size = self.layer_sizes[self.current_layer]\n",
    "        \n",
    "        new_usage = self.used_budget + layer_size * bit_value\n",
    "        \n",
    "        if new_usage > self.max_budget:\n",
    "            reward = torch.tensor(-1000.0, device=device)\n",
    "            done = True\n",
    "        else:\n",
    "            self.allocated_bits.append(bit_value.item())  # 记录时转CPU\n",
    "            self.used_budget = new_usage\n",
    "            self.current_layer += 1\n",
    "            reward = torch.tensor(0.1 if self.current_layer < self.n_layers else 0.0, device=device)\n",
    "            done = self.current_layer >= self.n_layers\n",
    "            \n",
    "            if done:\n",
    "                reward = self._calculate_final_reward()\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _calculate_final_reward(self):\n",
    "        \"\"\"GPU加速的奖励计算\"\"\"\n",
    "        allocated_bits_tensor = torch.tensor(self.allocated_bits, device=device)\n",
    "        loss = torch.sum(self.F * torch.exp(-self.alpha * allocated_bits_tensor))\n",
    "        return -loss\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.to(device)  # 确保网络在GPU上\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1))\n",
    "        self.to(device)  # 确保网络在GPU上\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.buffer = deque(maxlen=10000)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # 批量数据直接从GPU获取\n",
    "        batch = random.sample(self.buffer, BATCH_SIZE)\n",
    "        states = torch.stack([t[0] for t in batch]).to(device)\n",
    "        actions = torch.tensor([t[1] for t in batch], dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor([t[2] for t in batch], dtype=torch.float32, device=device)\n",
    "        next_states = torch.stack([t[3] for t in batch]).to(device)\n",
    "        dones = torch.tensor([t[4] for t in batch], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # 价值计算\n",
    "        with torch.no_grad():\n",
    "            target_v = rewards + GAMMA * (1 - dones) * self.critic(next_states).squeeze()\n",
    "        \n",
    "        # 优势计算\n",
    "        V = self.critic(states).squeeze()\n",
    "        advantage = (target_v - V).detach()\n",
    "        \n",
    "        # Critic更新\n",
    "        critic_loss = nn.MSELoss()(V, target_v)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # Actor更新\n",
    "        old_probs = self.actor(states).detach()\n",
    "        old_probs = old_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        for _ in range(UPDATE_ITERS):\n",
    "            new_probs = self.actor(states)\n",
    "            new_dist = torch.distributions.Categorical(new_probs)\n",
    "            log_probs = new_dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(log_probs - torch.log(old_probs))\n",
    "            clipped_ratio = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON)\n",
    "            \n",
    "            actor_loss = -torch.min(ratio*advantage, clipped_ratio*advantage).mean()\n",
    "            \n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "# 初始化环境和Agent\n",
    "env = BitAllocationEnv(layer_sizes, bits, F, alpha, R)\n",
    "state_dim = env._get_state().shape[0]\n",
    "action_dim = len(bits)\n",
    "agent = PPO(state_dim, action_dim)\n",
    "\n",
    "# 训练循环（完全GPU加速）\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # 存储GPU张量（注意：done需要转CPU）\n",
    "        agent.buffer.append((\n",
    "            state.cpu().detach(),  # 使用CPU存储减少显存占用\n",
    "            action,\n",
    "            reward.cpu().item(),\n",
    "            next_state.cpu().detach(),\n",
    "            done\n",
    "        ))\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward.item()\n",
    "    \n",
    "    # 更新策略\n",
    "    agent.update()\n",
    "    \n",
    "    # 打印进度\n",
    "    if (episode+1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}/{EPISODES} | Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# 测试阶段（保持GPU计算）\n",
    "with torch.no_grad():\n",
    "    test_env = BitAllocationEnv(layer_sizes, bits, F, alpha, R)\n",
    "    state = test_env.reset()\n",
    "    allocations = []\n",
    "    \n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = test_env.step(action)\n",
    "        allocations.append(bits[action])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "# 结果展示（转CPU处理）\n",
    "print(\"\\nFinal Bit Allocation:\")\n",
    "print(f\"Layers: {len(layer_sizes)}\")\n",
    "print(f\"Allocated Bits: {allocations}\")\n",
    "print(f\"Total Usage: {sum([l*b for l,b in zip(layer_sizes,allocations)])} bits\")\n",
    "print(f\"Original Usage: {test_env.original_size.item()} bits\")\n",
    "print(f\"Compression Rate: {np.sum(layer_sizes * (np.array(allocations) / 16))/np.sum(layer_sizes):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
